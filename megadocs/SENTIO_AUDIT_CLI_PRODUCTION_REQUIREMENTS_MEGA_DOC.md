# SENTIO_AUDIT_CLI_PRODUCTION_REQUIREMENTS_MEGA_DOC

**Generated**: 2025-09-11 14:13:20
**Source Directory**: /Users/yeogirlyun/C++/sentio_cpp
**Description**: Production-ready audit CLI requirements document for implementing sentio_audit with SQLite storage, hash-chain integrity, and canonical command structure

**Total Files**: 26

---

## üêõ **BUG REPORT**

# Sentio Audit CLI Production Requirements Document

## Overview
This document outlines the requirements for implementing a **production-ready, unified audit CLI** (`sentio_audit`) that provides canonical command options with SQLite-based storage, hash-chain integrity, and comprehensive audit analysis capabilities.

## Design Philosophy

### Core Principles
- **Single Binary**: One `sentio_audit` command for all audit operations
- **Hash-Chain Integrity**: Tamper-evident audit trail with cryptographic verification
- **SQLite Storage**: Durable, portable, high-performance database backend
- **Canonical Commands**: Non-overlapping, well-defined subcommands
- **Deterministic Replay**: Exact P&L verification using specified price series
- **Minimal Dependencies**: C++17 + system SQLite3 only

## Architecture Overview

### Database Schema (SQLite)
```sql
-- Core tables with foreign key constraints and indices
CREATE TABLE audit_runs (
  run_id      TEXT PRIMARY KEY,            -- canonical UID (UUID)
  started_at  INTEGER NOT NULL,            -- epoch millis
  ended_at    INTEGER,                     -- optional
  kind        TEXT NOT NULL,               -- backtest|paper|live
  strategy    TEXT NOT NULL,
  params_json TEXT NOT NULL,
  data_hash   TEXT NOT NULL,               -- hash(input universe/splits)
  git_rev     TEXT,
  note        TEXT
);

CREATE TABLE audit_events (
  run_id      TEXT NOT NULL,
  seq         INTEGER NOT NULL,            -- 1..N monotonic
  ts_millis   INTEGER NOT NULL,
  kind        TEXT NOT NULL,               -- SIGNAL|ORDER|FILL|PNL|NOTE
  symbol      TEXT,
  side        TEXT,                        -- BUY|SELL|NEUTRAL
  qty         REAL,
  price       REAL,
  pnl_delta   REAL,
  weight      REAL,                        -- signal weight
  prob        REAL,                        -- signal probability
  reason      TEXT,                        -- signal reason (e.g., RSI_LT_30)
  note        TEXT,
  hash_prev   TEXT NOT NULL,               -- previous event hash
  hash_curr   TEXT NOT NULL,                -- current event hash
  PRIMARY KEY (run_id, seq),
  FOREIGN KEY (run_id) REFERENCES audit_runs(run_id) ON DELETE CASCADE
);

CREATE TABLE audit_kv (
  run_id TEXT NOT NULL,
  k      TEXT NOT NULL,
  v      TEXT NOT NULL,
  PRIMARY KEY (run_id, k),
  FOREIGN KEY (run_id) REFERENCES audit_runs(run_id) ON DELETE CASCADE
);

-- Performance indices
CREATE INDEX idx_events_run_ts ON audit_events(run_id, ts_millis);
CREATE INDEX idx_events_kind ON audit_events(run_id, kind);
```

### Hash-Chain Integrity
```cpp
// Canonical content string for deterministic hashing
content = "run=<run_id>|seq=<seq>|ts=<ts>|kind=<kind>|symbol=<symbol>|side=<side>|qty=<qty>|price=<price>|pnl=<pnl>|weight=<w>|prob=<p>|reason=<reason>|note=<note>"

// Hash chain computation
hash_curr = SHA256(hash_prev + "\n" + SHA256(content))

// Genesis hash for first event
hash_prev = "GENESIS" (for seq=1)
```

## Required Canonical Commands

### 1. `init` - Database Initialization
**Purpose**: Initialize SQLite database with schema
**Usage**: `sentio_audit init --db <database.sqlite3>`

#### Features:
- Creates database file if it doesn't exist
- Initializes schema with tables, indices, and constraints
- Sets optimal SQLite pragmas (WAL mode, foreign keys, etc.)
- Atomic operation with rollback on failure

### 2. `new-run` - Create New Audit Run
**Purpose**: Start a new strategy run/backtest session
**Usage**: `sentio_audit new-run --db <db> --run <run_id> --strategy <name> --kind <backtest|paper|live> --params <file.json> --data-hash <hex> --git <rev> [--note <text>]`

#### Features:
- Creates canonical run ID (UUID)
- Records strategy parameters from JSON file
- Stores data hash for reproducibility
- Records git revision for traceability
- Timestamps run start

### 3. `log` - Append Audit Event
**Purpose**: Log individual audit events with hash-chain integrity
**Usage**: `sentio_audit log --db <db> --run <run_id> --ts <millis> --kind <SIGNAL|ORDER|FILL|PNL|NOTE> [--symbol <sym>] [--side <BUY|SELL|NEUTRAL>] [--qty <q>] [--price <p>] [--pnl <d>] [--weight <w>] [--prob <p>] [--reason <r>] [--note <t>]`

#### Features:
- Automatic sequence number assignment (monotonic)
- Hash-chain computation and verification
- Atomic insertion with rollback on failure
- Support for all event types (signals, orders, fills, P&L, notes)
- Returns sequence number and computed hash

### 4. `end-run` - Complete Audit Run
**Purpose**: Mark audit run as completed
**Usage**: `sentio_audit end-run --db <db> --run <run_id>`

#### Features:
- Records end timestamp
- Updates run status
- Triggers final verification checks

### 5. `verify` - Hash-Chain Verification
**Purpose**: Verify audit trail integrity and detect tampering
**Usage**: `sentio_audit verify --db <db> --run <run_id>`

#### Features:
- Verifies hash-chain integrity for entire run
- Checks sequence monotonicity
- Validates foreign key constraints
- Detects any data tampering or corruption
- Returns detailed verification results

### 6. `summarize` - Run Summary Statistics
**Purpose**: Generate comprehensive run statistics
**Usage**: `sentio_audit summarize --db <db> --run <run_id>`

#### Features:
- Event counts by type (signals, orders, fills, P&L)
- Total P&L sum
- Time range (first/last timestamps)
- Performance metrics
- Quick overview for debugging

### 7. `export` - Data Export
**Purpose**: Export audit data in various formats
**Usage**: `sentio_audit export --db <db> --run <run_id> --fmt <jsonl|csv> --out <file>`

#### Features:
- CSV export with proper escaping
- JSONL export with JSON formatting
- Streaming export for large datasets
- Preserves all hash-chain data
- Atomic file writing

### 8. `grep` - Ad-hoc Querying
**Purpose**: Query audit events with SQL WHERE clauses
**Usage**: `sentio_audit grep --db <db> --run <run_id> --sql "WHERE kind='FILL' AND price>0"`

#### Features:
- SQL WHERE clause support
- Human-readable output format
- Fast indexed queries
- Flexible filtering capabilities
- Returns row count

### 9. `diff` - Run Comparison
**Purpose**: Compare two audit runs
**Usage**: `sentio_audit diff --db <db> --run <run_a> --run2 <run_b>`

#### Features:
- Side-by-side run comparison
- Event count differences
- P&L differences
- Time range comparison
- Summary statistics comparison

### 10. `replay` - Deterministic P&L Replay
**Purpose**: Replay trades using exact price series for verification
**Usage**: `sentio_audit replay --db <db> --run <run_id> --prices <price.csv> --tz <NY> [--check-pnl]`

#### Features:
- Loads price series from CSV
- Replays all fills chronologically
- Computes P&L using exact symbol prices
- Validates against stored P&L
- Detects symbol mapping errors
- Timezone-aware processing

### 11. `vacuum` - Database Maintenance
**Purpose**: Optimize database performance
**Usage**: `sentio_audit vacuum --db <db>`

#### Features:
- SQLite VACUUM operation
- Rebuilds indices
- Optimizes storage
- Analyzes query performance

## Implementation Architecture

### Core Components

#### 1. Database Layer (`audit_db.hpp/cpp`)
```cpp
namespace audit {
class DB {
public:
  explicit DB(const std::string& path);
  ~DB();
  
  void init_schema();
  void new_run(const RunRow& run);
  void end_run(const std::string& run_id, std::int64_t ended_at);
  
  // Hash-chain aware event logging
  std::pair<std::int64_t, std::string> append_event(const Event& ev);
  
  // Verification and analysis
  std::pair<bool, std::string> verify_run(const std::string& run_id);
  Summary summarize(const std::string& run_id);
  
  // Export and querying
  void export_run_csv(const std::string& run_id, const std::string& out_path);
  void export_run_jsonl(const std::string& run_id, const std::string& out_path);
  long grep_where(const std::string& run_id, const std::string& where_sql);
  std::string diff_runs(const std::string& run_a, const std::string& run_b);
  
  void vacuum();
};
}
```

#### 2. Hash Computation (`hash.hpp/cpp`)
```cpp
namespace audit {
// SHA-256 implementation (public domain)
std::string sha256_hex(const void* data, size_t n);
inline std::string sha256_hex(const std::string& s);
}
```

#### 3. CLI Interface (`audit_cli.hpp/cpp`)
```cpp
namespace audit {
int audit_main(int argc, char** argv);
}
```

#### 4. Price Series Support (`price_csv.hpp/cpp`)
```cpp
namespace audit {
struct Bar { std::int64_t ts; double open, high, low, close, volume; };
using Series = std::vector<Bar>;
std::unordered_map<std::string, Series> load_price_csv(const std::string& path);
}
```

### File Structure
```
audit/
‚îú‚îÄ‚îÄ include/audit/
‚îÇ   ‚îú‚îÄ‚îÄ audit_cli.hpp
‚îÇ   ‚îú‚îÄ‚îÄ audit_db.hpp
‚îÇ   ‚îú‚îÄ‚îÄ hash.hpp
‚îÇ   ‚îú‚îÄ‚îÄ clock.hpp
‚îÇ   ‚îî‚îÄ‚îÄ price_csv.hpp
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ audit_db.cpp
‚îÇ   ‚îú‚îÄ‚îÄ audit_cli.cpp
‚îÇ   ‚îú‚îÄ‚îÄ hash.cpp
‚îÇ   ‚îî‚îÄ‚îÄ price_csv.cpp
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_verify.cpp
‚îî‚îÄ‚îÄ CMakeLists.txt
```

## Integration with Existing Sentio System

### Migration from Current Audit System
1. **Phase 1**: Implement new SQLite-based audit CLI alongside existing system
2. **Phase 2**: Add migration tools to convert existing JSONL audit files
3. **Phase 3**: Update C++ audit recording to use new database format
4. **Phase 4**: Deprecate old audit system after transition period

### C++ Integration Points
- **`src/audit.cpp`**: Update to use new database format
- **`include/sentio/audit.hpp`**: Extend with new event types
- **`src/runner.cpp`**: Integrate with new audit logging
- **`src/signal_engine.cpp`**: Add signal audit logging
- **`src/signal_pipeline.cpp`**: Add pipeline audit logging

### Python Integration
- **`tools/audit_cli.py`**: Deprecate in favor of C++ CLI
- **`tools/audit_analyzer.py`**: Update to read from SQLite
- **`tools/audit_parser.py`**: Update for new format

## Performance Characteristics

### Expected Performance
- **Event Logging**: >200k events/second
- **Hash Computation**: <1Œºs per event
- **Verification**: <100ms for 1M events
- **Query Performance**: Sub-millisecond for indexed queries
- **Export**: >50MB/second for CSV/JSONL

### Scalability
- **Database Size**: Tested up to 100M events
- **Concurrent Access**: SQLite WAL mode supports multiple readers
- **Memory Usage**: <100MB for typical audit runs
- **Storage**: ~200 bytes per event (including hash chain)

## Security and Integrity

### Tamper Detection
- **Hash-Chain Integrity**: Any modification breaks subsequent hashes
- **Cryptographic Verification**: SHA-256 for all hash computations
- **Deterministic Canonicalization**: Same content produces same hash
- **Foreign Key Constraints**: Prevents orphaned records

### Data Validation
- **Sequence Monotonicity**: Enforced at database level
- **Type Validation**: SQLite type system enforcement
- **Constraint Checking**: Foreign keys and NOT NULL constraints
- **NaN Detection**: Guards against invalid floating-point values

## Testing Requirements

### Unit Tests
- Hash computation correctness
- Database schema validation
- Event logging and retrieval
- Hash-chain verification
- Export format validation

### Integration Tests
- End-to-end audit workflow
- Large dataset handling
- Concurrent access patterns
- Migration from existing format
- Performance benchmarks

### Verification Tests
- Tamper detection validation
- P&L replay accuracy
- Cross-platform compatibility
- Memory leak detection

## Migration Strategy

### Phase 1: Core Implementation (Week 1-2)
1. Implement database schema and core classes
2. Implement hash computation and verification
3. Implement basic CLI commands (init, new-run, log, verify)
4. Add comprehensive unit tests

### Phase 2: Advanced Features (Week 3)
1. Implement export functionality (CSV, JSONL)
2. Implement query and diff capabilities
3. Implement replay functionality
4. Add performance optimizations

### Phase 3: Integration (Week 4)
1. Integrate with existing C++ audit system
2. Create migration tools for existing data
3. Update Python tools to use new format
4. Comprehensive integration testing

### Phase 4: Deployment (Week 5)
1. Production deployment
2. Performance monitoring
3. User training and documentation
4. Deprecation of old system

## Success Criteria

### Functional Requirements
- ‚úÖ Single binary (`sentio_audit`) for all audit operations
- ‚úÖ Hash-chain integrity with tamper detection
- ‚úÖ All canonical commands working correctly
- ‚úÖ Deterministic P&L replay verification
- ‚úÖ High-performance event logging (>200k/sec)
- ‚úÖ Complete migration from existing system

### Quality Requirements
- ‚úÖ 100% test coverage for core functionality
- ‚úÖ Memory leak-free operation
- ‚úÖ Cross-platform compatibility (Linux, macOS, Windows)
- ‚úÖ Comprehensive documentation
- ‚úÖ Performance benchmarks met

### User Experience Requirements
- ‚úÖ Intuitive command-line interface
- ‚úÖ Clear error messages and validation
- ‚úÖ Fast execution times
- ‚úÖ Consistent output formatting
- ‚úÖ Easy integration with existing workflows

## Dependencies

### System Requirements
- **C++17**: Standard library features
- **SQLite3**: System library (>=3.35.0)
- **CMake**: Build system (>=3.16)
- **SHA-256**: Public domain implementation

### Optional Dependencies
- **Parquet**: For large dataset export (future enhancement)
- **Rust FFI**: For Rust integration (future enhancement)

## Conclusion

This production-ready audit CLI design provides a comprehensive, unified solution for audit trail management with cryptographic integrity, high performance, and canonical command structure. The SQLite-based approach ensures portability and durability while the hash-chain mechanism provides tamper-evident audit trails.

The design eliminates the current scattered approach of multiple Python scripts and provides a single, professional interface for all audit operations. The deterministic replay capability ensures correctness verification, while the flexible querying and export capabilities support comprehensive analysis workflows.

This implementation will transform Sentio's audit system from a collection of ad-hoc scripts into a production-grade, enterprise-ready audit platform that scales with the project's needs and provides the reliability and integrity required for financial trading systems.


---

## üìã **TABLE OF CONTENTS**

1. [tools/align_bars.py](#file-1)
2. [tools/analyze_leverage_trading.py](#file-2)
3. [tools/analyze_trades_csv.py](#file-3)
4. [tools/audit_analyzer.py](#file-4)
5. [tools/audit_chain_report.py](#file-5)
6. [tools/audit_cli.py](#file-6)
7. [tools/audit_parser.py](#file-7)
8. [tools/compare_real_vs_virtual.cpp](#file-8)
9. [tools/create_mega_document.py](#file-9)
10. [tools/csv_runner.cpp](#file-10)
11. [tools/data_downloader.py](#file-11)
12. [tools/dupdef_scan_cpp.py](#file-12)
13. [tools/emit_last10_trades.py](#file-13)
14. [tools/extract_instrument_distribution.py](#file-14)
15. [tools/fast_historical_bridge.py](#file-15)
16. [tools/finalize_kochi_features.py](#file-16)
17. [tools/generate_bar_sequence.cpp](#file-17)
18. [tools/generate_feature_cache.py](#file-18)
19. [tools/generate_kochi_feature_cache.py](#file-19)
20. [tools/historical_context_agent.py](#file-20)
21. [tools/ire_param_sweep.cpp](#file-21)
22. [tools/kochi_bin_runner.cpp](#file-22)
23. [tools/mars_bridge.py](#file-23)
24. [tools/replay_audit.cpp](#file-24)
25. [tools/tfa_sanity_check.py](#file-25)
26. [tools/tfa_sanity_check_report.txt](#file-26)

---

## üìÑ **FILE 1 of 26**: tools/align_bars.py

**File Information**:
- **Path**: `tools/align_bars.py`

- **Size**: 113 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
import argparse
import pathlib
import sys
from typing import Tuple


def read_bars(path: pathlib.Path):
    import pandas as pd
    # Try with header detection; polygon files often have no header
    try:
        df = pd.read_csv(path, header=None)
        # Heuristic: 7 columns: ts,symbol,open,high,low,close,volume
        if df.shape[1] < 7:
            # Retry with header row
            df = pd.read_csv(path)
    except Exception:
        df = pd.read_csv(path)

    if df.shape[1] >= 7:
        cols = ["ts", "symbol", "open", "high", "low", "close", "volume"] + [f"extra{i}" for i in range(df.shape[1]-7)]
        df.columns = cols[:df.shape[1]]
    elif df.shape[1] == 6:
        df.columns = ["ts", "open", "high", "low", "close", "volume"]
        df["symbol"] = path.stem.split("_")[0]
        df = df[["ts","symbol","open","high","low","close","volume"]]
    else:
        raise ValueError(f"Unexpected column count in {path}: {df.shape[1]}")

    # Normalize ts to string and index
    df["ts"] = df["ts"].astype(str)
    df = df.set_index("ts").sort_index()
    return df


def align_intersection(df1, df2, df3, df4=None):
    idx = df1.index.intersection(df2.index).intersection(df3.index)
    if df4 is not None:
        idx = idx.intersection(df4.index)
    idx = idx.sort_values()
    if df4 is not None:
        return df1.loc[idx], df2.loc[idx], df3.loc[idx], df4.loc[idx]
    else:
        return df1.loc[idx], df2.loc[idx], df3.loc[idx]


def write_bars(path: pathlib.Path, df) -> None:
    # Preserve original polygon-like format: ts,symbol,open,high,low,close,volume
    out = df.reset_index()[["ts","symbol","open","high","low","close","volume"]]
    out.to_csv(path, index=False)


def derive_out(path: pathlib.Path, suffix: str) -> pathlib.Path:
    stem = path.stem
    if stem.endswith(".csv"):
        stem = stem[:-4]
    return path.with_name(f"{stem}_{suffix}.csv")


def main():
    ap = argparse.ArgumentParser(description="Align QQQ/TQQQ/SQQQ/PSQ minute bars by timestamp intersection.")
    ap.add_argument("--qqq", required=True)
    ap.add_argument("--tqqq", required=True)
    ap.add_argument("--sqqq", required=True)
    ap.add_argument("--psq", required=False, help="Optional PSQ data file")
    ap.add_argument("--suffix", default="ALIGNED")
    args = ap.parse_args()

    qqq_p = pathlib.Path(args.qqq)
    tqqq_p = pathlib.Path(args.tqqq)
    sqqq_p = pathlib.Path(args.sqqq)
    psq_p = pathlib.Path(args.psq) if args.psq else None

    import pandas as pd
    pd.options.mode.chained_assignment = None

    df_q = read_bars(qqq_p)
    df_t = read_bars(tqqq_p)
    df_s = read_bars(sqqq_p)
    df_p = read_bars(psq_p) if psq_p else None

    if df_p is not None:
        a_q, a_t, a_s, a_p = align_intersection(df_q, df_t, df_s, df_p)
        assert list(a_q.index) == list(a_t.index) == list(a_s.index) == list(a_p.index)
    else:
        a_q, a_t, a_s = align_intersection(df_q, df_t, df_s)
        assert list(a_q.index) == list(a_t.index) == list(a_s.index)

    out_q = derive_out(qqq_p, args.suffix)
    out_t = derive_out(tqqq_p, args.suffix)
    out_s = derive_out(sqqq_p, args.suffix)

    write_bars(out_q, a_q)
    write_bars(out_t, a_t)
    write_bars(out_s, a_s)

    print_files = [f"‚Üí {out_q}", f"‚Üí {out_t}", f"‚Üí {out_s}"]

    if df_p is not None:
        out_p = derive_out(psq_p, args.suffix)
        write_bars(out_p, a_p)
        print_files.append(f"‚Üí {out_p}")

    n = len(a_q)
    print(f"Aligned bars: {n}")
    for file_path in print_files:
        print(file_path)


if __name__ == "__main__":
    main()



```

## üìÑ **FILE 2 of 26**: tools/analyze_leverage_trading.py

**File Information**:
- **Path**: `tools/analyze_leverage_trading.py`

- **Size**: 220 lines
- **Modified**: 2025-09-10 14:31:06

- **Type**: .py

```text
#!/usr/bin/env python3
"""
Analyze leverage trading patterns in IRE and ASP strategies.
Focuses on instrument distribution (QQQ, PSQ, TQQQ, SQQQ) and P/L analysis.
"""

import argparse
import json
import sys
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import statistics

def load_audit_file(file_path):
    """Load and parse audit JSONL file."""
    events = []
    with open(file_path, 'r') as f:
        for line in f:
            try:
                # Extract JSON part before the sha1 field
                json_part = line.strip()
                if '","sha1":"' in json_part:
                    json_part = json_part.split('","sha1":"')[0] + '"}'
                elif ',"sha1":"' in json_part:
                    json_part = json_part.split(',"sha1":"')[0] + '}'
                
                event = json.loads(json_part)
                events.append(event)
            except json.JSONDecodeError:
                continue
    return events

def analyze_leverage_trading(events):
    """Analyze leverage trading patterns from audit events."""
    
    # Separate different event types (using actual audit format)
    trades = [e for e in events if e.get('type') == 'fill']
    signals = [e for e in events if e.get('type') == 'signal']
    orders = [e for e in events if e.get('type') == 'order']
    
    print(f"üìä ANALYSIS SUMMARY")
    print(f"Total Events: {len(events)}")
    print(f"Trades: {len(trades)}")
    print(f"Signals: {len(signals)}")
    print(f"Orders: {len(orders)}")
    print()
    
    # Analyze instrument distribution in trades
    instrument_trades = defaultdict(list)
    instrument_stats = defaultdict(lambda: {
        'total_trades': 0,
        'buy_trades': 0,
        'sell_trades': 0,
        'total_qty': 0,
        'total_notional': 0,
        'pnl_by_trade': [],
        'winning_trades': 0,
        'losing_trades': 0
    })
    
    for trade in trades:
        instrument = trade.get('inst', 'UNKNOWN')
        side = trade.get('side', 0)  # 0=sell, 1=buy
        qty = trade.get('qty', 0)
        px = trade.get('px', 0)
        pnl_d = trade.get('pnl_d', 0)
        
        instrument_trades[instrument].append(trade)
        
        stats = instrument_stats[instrument]
        stats['total_trades'] += 1
        stats['total_qty'] += qty
        stats['total_notional'] += qty * px
        
        if side == 1:  # Buy
            stats['buy_trades'] += 1
        else:  # Sell
            stats['sell_trades'] += 1
            
        stats['pnl_by_trade'].append(pnl_d)
        
        if pnl_d > 0:
            stats['winning_trades'] += 1
        elif pnl_d < 0:
            stats['losing_trades'] += 1
    
    # Analyze signal routing patterns from orders
    signal_routing = defaultdict(lambda: defaultdict(int))
    for order in orders:
        instrument = order.get('inst', 'UNKNOWN')
        side = order.get('side', 0)  # 0=sell, 1=buy
        
        if side == 1:  # Buy
            signal_routing[instrument]['long_signals'] += 1
        elif side == 0:  # Sell
            signal_routing[instrument]['short_signals'] += 1
        else:
            signal_routing[instrument]['neutral_signals'] += 1
    
    # Print instrument distribution analysis
    print("üéØ INSTRUMENT DISTRIBUTION ANALYSIS")
    print("=" * 60)
    
    total_trades = sum(stats['total_trades'] for stats in instrument_stats.values())
    
    for instrument in sorted(instrument_stats.keys()):
        stats = instrument_stats[instrument]
        trades_count = stats['total_trades']
        percentage = (trades_count / total_trades * 100) if total_trades > 0 else 0
        
        print(f"\nüìà {instrument} Analysis:")
        print(f"  Total Trades: {trades_count} ({percentage:.1f}%)")
        print(f"  Buy Trades: {stats['buy_trades']}")
        print(f"  Sell Trades: {stats['sell_trades']}")
        print(f"  Total Quantity: {stats['total_qty']:,.0f}")
        print(f"  Total Notional: ${stats['total_notional']:,.2f}")
        
        if stats['pnl_by_trade']:
            total_pnl = sum(stats['pnl_by_trade'])
            avg_pnl = statistics.mean(stats['pnl_by_trade'])
            win_rate = (stats['winning_trades'] / trades_count * 100) if trades_count > 0 else 0
            
            print(f"  Total P&L: ${total_pnl:,.2f}")
            print(f"  Average P&L per Trade: ${avg_pnl:,.2f}")
            print(f"  Win Rate: {win_rate:.1f}% ({stats['winning_trades']}/{trades_count})")
            print(f"  Winning Trades: {stats['winning_trades']}")
            print(f"  Losing Trades: {stats['losing_trades']}")
            
            if len(stats['pnl_by_trade']) > 1:
                pnl_std = statistics.stdev(stats['pnl_by_trade'])
                print(f"  P&L Std Dev: ${pnl_std:,.2f}")
    
    # Print signal routing analysis
    print(f"\nüö¶ SIGNAL ROUTING ANALYSIS")
    print("=" * 60)
    
    for instrument in sorted(signal_routing.keys()):
        routing = signal_routing[instrument]
        total_signals = sum(routing.values())
        
        if total_signals > 0:
            print(f"\nüì° {instrument} Signal Routing:")
            print(f"  Total Signals: {total_signals}")
            print(f"  Long Signals: {routing['long_signals']} ({routing['long_signals']/total_signals*100:.1f}%)")
            print(f"  Short Signals: {routing['short_signals']} ({routing['short_signals']/total_signals*100:.1f}%)")
            print(f"  Neutral Signals: {routing['neutral_signals']} ({routing['neutral_signals']/total_signals*100:.1f}%)")
    
    # Analyze leverage patterns
    print(f"\n‚ö° LEVERAGE TRADING ANALYSIS")
    print("=" * 60)
    
    leverage_instruments = {
        'QQQ': '1x Long (Base)',
        'TQQQ': '3x Long (Bull)',
        'SQQQ': '3x Short (Bear)',
        'PSQ': '1x Short (Inverse)'
    }
    
    for instrument, description in leverage_instruments.items():
        if instrument in instrument_stats:
            stats = instrument_stats[instrument]
            trades_count = stats['total_trades']
            percentage = (trades_count / total_trades * 100) if total_trades > 0 else 0
            
            print(f"\nüî∏ {instrument} ({description}):")
            print(f"  Usage: {trades_count} trades ({percentage:.1f}%)")
            
            if stats['pnl_by_trade']:
                total_pnl = sum(stats['pnl_by_trade'])
                win_rate = (stats['winning_trades'] / trades_count * 100) if trades_count > 0 else 0
                print(f"  P&L: ${total_pnl:,.2f}")
                print(f"  Win Rate: {win_rate:.1f}%")
    
    # Analyze PSQ usage specifically (1x reverse trades)
    if 'PSQ' in instrument_stats:
        psq_stats = instrument_stats['PSQ']
        print(f"\nüîÑ PSQ (1x REVERSE) ANALYSIS")
        print("=" * 60)
        print(f"PSQ represents 1x inverse QQQ trades (short exposure)")
        print(f"Total PSQ Trades: {psq_stats['total_trades']}")
        print(f"PSQ Trade Percentage: {psq_stats['total_trades']/total_trades*100:.1f}%")
        
        if psq_stats['pnl_by_trade']:
            psq_pnl = sum(psq_stats['pnl_by_trade'])
            psq_win_rate = (psq_stats['winning_trades'] / psq_stats['total_trades'] * 100) if psq_stats['total_trades'] > 0 else 0
            print(f"PSQ Total P&L: ${psq_pnl:,.2f}")
            print(f"PSQ Win Rate: {psq_win_rate:.1f}%")
    
    return instrument_stats, signal_routing

def main():
    parser = argparse.ArgumentParser(description='Analyze leverage trading patterns')
    parser.add_argument('audit_file', help='Path to audit JSONL file')
    parser.add_argument('--strategy', help='Strategy name for context')
    
    args = parser.parse_args()
    
    audit_file = Path(args.audit_file)
    if not audit_file.exists():
        print(f"Error: Audit file '{audit_file}' not found.")
        sys.exit(1)
    
    print(f"üîç Analyzing leverage trading patterns...")
    print(f"üìÅ File: {audit_file}")
    if args.strategy:
        print(f"üìä Strategy: {args.strategy}")
    print()
    
    events = load_audit_file(audit_file)
    if not events:
        print("Error: No valid events found in audit file.")
        sys.exit(1)
    
    instrument_stats, signal_routing = analyze_leverage_trading(events)
    
    print(f"\n‚úÖ Analysis complete!")

if __name__ == '__main__':
    main()

```

## üìÑ **FILE 3 of 26**: tools/analyze_trades_csv.py

**File Information**:
- **Path**: `tools/analyze_trades_csv.py`

- **Size**: 136 lines
- **Modified**: 2025-09-10 14:39:37

- **Type**: .py

```text
#!/usr/bin/env python3
"""
Analyze trades CSV to understand leverage trading patterns.
"""

import pandas as pd
import sys
from collections import defaultdict

def analyze_trades_csv(csv_file):
    """Analyze trades CSV for leverage trading patterns."""
    
    # Read the CSV
    df = pd.read_csv(csv_file)
    
    print(f"üìä TRADES ANALYSIS")
    print(f"Total Trades: {len(df)}")
    print()
    
    # Analyze by instrument
    instrument_stats = {}
    
    for instrument in df['inst'].unique():
        inst_df = df[df['inst'] == instrument]
        
        stats = {
            'total_trades': len(inst_df),
            'buy_trades': len(inst_df[inst_df['side'] == 1]),
            'sell_trades': len(inst_df[inst_df['side'] == 0]),
            'total_qty': inst_df['qty'].sum(),
            'total_notional': (inst_df['qty'] * inst_df['px']).sum(),
            'total_pnl': inst_df['pnl_d'].sum(),
            'avg_pnl': inst_df['pnl_d'].mean(),
            'winning_trades': len(inst_df[inst_df['pnl_d'] > 0]),
            'losing_trades': len(inst_df[inst_df['pnl_d'] < 0]),
            'win_rate': len(inst_df[inst_df['pnl_d'] > 0]) / len(inst_df) * 100 if len(inst_df) > 0 else 0
        }
        
        instrument_stats[instrument] = stats
    
    # Print analysis
    print("üéØ INSTRUMENT DISTRIBUTION ANALYSIS")
    print("=" * 60)
    
    total_trades = len(df)
    
    for instrument in sorted(instrument_stats.keys()):
        stats = instrument_stats[instrument]
        percentage = stats['total_trades'] / total_trades * 100
        
        print(f"\nüìà {instrument} Analysis:")
        print(f"  Total Trades: {stats['total_trades']} ({percentage:.1f}%)")
        print(f"  Buy Trades: {stats['buy_trades']}")
        print(f"  Sell Trades: {stats['sell_trades']}")
        print(f"  Total Quantity: {stats['total_qty']:,.0f}")
        print(f"  Total Notional: ${stats['total_notional']:,.2f}")
        print(f"  Total P&L: ${stats['total_pnl']:,.2f}")
        print(f"  Average P&L per Trade: ${stats['avg_pnl']:,.2f}")
        print(f"  Win Rate: {stats['win_rate']:.1f}% ({stats['winning_trades']}/{stats['total_trades']})")
        print(f"  Winning Trades: {stats['winning_trades']}")
        print(f"  Losing Trades: {stats['losing_trades']}")
    
    # Analyze leverage patterns
    print(f"\n‚ö° LEVERAGE TRADING ANALYSIS")
    print("=" * 60)
    
    leverage_instruments = {
        'QQQ': '1x Long (Base)',
        'TQQQ': '3x Long (Bull)',
        'SQQQ': '3x Short (Bear)',
        'PSQ': '1x Short (Inverse)'
    }
    
    for instrument, description in leverage_instruments.items():
        if instrument in instrument_stats:
            stats = instrument_stats[instrument]
            percentage = stats['total_trades'] / total_trades * 100
            
            print(f"\nüî∏ {instrument} ({description}):")
            print(f"  Usage: {stats['total_trades']} trades ({percentage:.1f}%)")
            print(f"  P&L: ${stats['total_pnl']:,.2f}")
            print(f"  Win Rate: {stats['win_rate']:.1f}%")
    
    # Analyze PSQ usage specifically (1x reverse trades)
    if 'PSQ' in instrument_stats:
        psq_stats = instrument_stats['PSQ']
        print(f"\nüîÑ PSQ (1x REVERSE) ANALYSIS")
        print("=" * 60)
        print(f"PSQ represents 1x inverse QQQ trades (short exposure)")
        print(f"Total PSQ Trades: {psq_stats['total_trades']}")
        print(f"PSQ Trade Percentage: {psq_stats['total_trades']/total_trades*100:.1f}%")
        print(f"PSQ Total P&L: ${psq_stats['total_pnl']:,.2f}")
        print(f"PSQ Win Rate: {psq_stats['win_rate']:.1f}%")
        
        # Show some PSQ trade examples
        psq_trades = df[df['inst'] == 'PSQ'].head(10)
        if len(psq_trades) > 0:
            print(f"\nüìã Sample PSQ Trades:")
            for _, trade in psq_trades.iterrows():
                side_str = "BUY" if trade['side'] == 1 else "SELL"
                print(f"  {side_str} {trade['qty']:.0f} shares @ ${trade['px']:.2f} | P&L: ${trade['pnl_d']:.2f}")
    
    # Analyze trading patterns
    print(f"\nüìà TRADING PATTERNS")
    print("=" * 60)
    
    # Side distribution
    buy_trades = len(df[df['side'] == 1])
    sell_trades = len(df[df['side'] == 0])
    print(f"Buy Trades: {buy_trades} ({buy_trades/total_trades*100:.1f}%)")
    print(f"Sell Trades: {sell_trades} ({sell_trades/total_trades*100:.1f}%)")
    
    # Overall performance
    total_pnl = df['pnl_d'].sum()
    avg_pnl = df['pnl_d'].mean()
    win_rate = len(df[df['pnl_d'] > 0]) / total_trades * 100
    
    print(f"\nOverall Performance:")
    print(f"Total P&L: ${total_pnl:,.2f}")
    print(f"Average P&L per Trade: ${avg_pnl:,.2f}")
    print(f"Overall Win Rate: {win_rate:.1f}%")
    
    return instrument_stats

def main():
    if len(sys.argv) != 2:
        print("Usage: python analyze_trades_csv.py <trades_csv_file>")
        sys.exit(1)
    
    csv_file = sys.argv[1]
    instrument_stats = analyze_trades_csv(csv_file)
    
    print(f"\n‚úÖ Analysis complete!")

if __name__ == '__main__':
    main()

```

## üìÑ **FILE 4 of 26**: tools/audit_analyzer.py

**File Information**:
- **Path**: `tools/audit_analyzer.py`

- **Size**: 254 lines
- **Modified**: 2025-09-10 13:43:01

- **Type**: .py

```text
from __future__ import annotations
import csv, sys, pathlib
from typing import Dict, Any, List, Optional, Tuple
from audit_parser import iter_audit_file
from datetime import datetime, timezone
from collections import defaultdict

# --- Minimal "schema" normalization (no pydantic dependency) ---

def normalize_event(e: Dict[str, Any]) -> Dict[str, Any]:
    # Ensure required keys exist; fill safe defaults
    e.setdefault("type", "")
    e.setdefault("run", "")
    e.setdefault("seq", 0)
    e.setdefault("ts", 0)  # epoch millis or nanos per your writer
    return e

# --- Analyzer ---

class AuditAnalyzer:
    def __init__(self) -> None:
        self.trades: List[Dict[str, Any]] = []
        self.snapshots: List[Dict[str, Any]] = []
        self.signals: List[Dict[str, Any]] = []
        self.bars: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.run_metadata: Dict[str, Any] = {}
        self.other: List[Dict[str, Any]] = []

    def load(self, path: str | pathlib.Path) -> None:
        for ln, event, sha1 in iter_audit_file(path):
            e = normalize_event(event)
            if sha1 is not None:
                e["_sha1"] = sha1  # keep association; optional
            t = e.get("type", "")
            try:
                if t == "fill":
                    self.trades.append(e)
                elif t == "snapshot":
                    self.snapshots.append(e)
                elif t == "signal":
                    self.signals.append(e)
                elif t == "bar":
                    inst = e.get("inst", "unknown")
                    self.bars[inst].append(e)
                elif t == "run_start":
                    self.run_metadata = e.get("meta", {})
                    self.other.append(e)
                else:
                    self.other.append(e)
            except Exception as ex:
                # Soft-fail the line, keep going
                if ln <= 5:
                    print(f"‚ö†Ô∏è  Processing error on line {ln}: {ex}", file=sys.stderr)

    def stats(self) -> Dict[str, int]:
        return {
            "trades": len(self.trades),
            "snapshots": len(self.snapshots),
            "signals": len(self.signals),
            "bars": sum(len(bars) for bars in self.bars.values()),
            "other": len(self.other),
        }

    def analyze_strategy_performance(self) -> dict:
        """Analyze strategy performance from audit trail"""
        if not self.snapshots:
            return {"error": "No snapshots found"}
        
        # Calculate key metrics
        initial_equity = self.snapshots[0].get("equity", 100000.0) if self.snapshots else 100000.0
        final_equity = self.snapshots[-1].get("equity", initial_equity) if self.snapshots else initial_equity
        total_return = (final_equity - initial_equity) / initial_equity * 100
        
        # Trade analysis
        total_trades = len(self.trades)
        buy_trades = len([t for t in self.trades if t.get("side") == 1 or t.get("side") == "Buy"])
        sell_trades = len([t for t in self.trades if t.get("side") == 0 or t.get("side") == "Sell"])
        
        # Daily analysis
        daily_trades = self._analyze_daily_trades()
        
        return {
            "strategy": self.run_metadata.get("strategy", "Unknown"),
            "initial_equity": initial_equity,
            "final_equity": final_equity,
            "total_return_pct": total_return,
            "total_trades": total_trades,
            "buy_trades": buy_trades,
            "sell_trades": sell_trades,
            "daily_trades": daily_trades,
            "snapshots_count": len(self.snapshots),
            "signals_count": len(self.signals)
        }
    
    def _analyze_daily_trades(self) -> List[dict]:
        """Analyze trades by day"""
        daily_data = defaultdict(lambda: {'trades': 0, 'volume': 0.0, 'instruments': set()})
        
        for trade in self.trades:
            # Convert timestamp to date (assuming UTC epoch)
            timestamp = trade.get("ts", 0)
            if timestamp > 0:
                date = datetime.fromtimestamp(timestamp, tz=timezone.utc).date()
                daily_data[date]['trades'] += 1
                qty = trade.get("qty", 0)
                price = trade.get("px", 0)
                daily_data[date]['volume'] += abs(qty * price)
                inst = trade.get("inst", "")
                if inst:
                    daily_data[date]['instruments'].add(inst)
        
        # Convert to list and sort by date
        daily_list = []
        for date in sorted(daily_data.keys()):
            data = daily_data[date]
            daily_list.append({
                'date': str(date),
                'trades': data['trades'],
                'volume': data['volume'],
                'instruments': list(data['instruments'])
            })
        
        return daily_list
    
    def get_daily_balance_changes(self) -> List[dict]:
        """Get daily balance changes from snapshots"""
        daily_balances = defaultdict(list)
        
        for snapshot in self.snapshots:
            timestamp = snapshot.get("ts", 0)
            if timestamp > 0:
                date = datetime.fromtimestamp(timestamp, tz=timezone.utc).date()
                daily_balances[date].append(snapshot)
        
        daily_changes = []
        for date in sorted(daily_balances.keys()):
            snapshots = daily_balances[date]
            if snapshots:
                # Use the last snapshot of the day
                last_snapshot = snapshots[-1]
                daily_changes.append({
                    'date': str(date),
                    'cash': last_snapshot.get("cash", 0),
                    'equity': last_snapshot.get("equity", 0),
                    'realized': last_snapshot.get("realized", 0),
                    'snapshots': len(snapshots)
                })
        
        return daily_changes

    def print_summary(self):
        """Print a comprehensive summary of the audit trail"""
        print(f"\n{'='*60}")
        print(f"üìä AUDIT TRAIL ANALYSIS SUMMARY")
        print(f"{'='*60}")
        
        # Strategy info
        strategy = self.run_metadata.get("strategy", "Unknown")
        print(f"üéØ Strategy: {strategy}")
        
        # Performance analysis
        perf = self.analyze_strategy_performance()
        if 'error' not in perf:
            print(f"\nüí∞ PERFORMANCE METRICS:")
            print(f"   Initial Equity: ${perf['initial_equity']:,.2f}")
            print(f"   Final Equity:   ${perf['final_equity']:,.2f}")
            print(f"   Total Return:   {perf['total_return_pct']:.2f}%")
            print(f"   Total Trades:   {perf['total_trades']:,}")
            print(f"   Buy Trades:     {perf['buy_trades']:,}")
            print(f"   Sell Trades:    {perf['sell_trades']:,}")
            print(f"   Signals:        {perf['signals_count']:,}")
        
        # Daily analysis
        daily_trades = perf.get('daily_trades', [])
        if daily_trades:
            avg_daily_trades = sum(d['trades'] for d in daily_trades) / len(daily_trades)
            print(f"\nüìà DAILY ANALYSIS:")
            print(f"   Trading Days:   {len(daily_trades)}")
            print(f"   Avg Daily Trades: {avg_daily_trades:.1f}")
            print(f"   Max Daily Trades: {max(d['trades'] for d in daily_trades)}")
            print(f"   Min Daily Trades: {min(d['trades'] for d in daily_trades)}")
        
        # Balance changes
        daily_balances = self.get_daily_balance_changes()
        if daily_balances:
            print(f"\nüí≥ DAILY BALANCE CHANGES:")
            print(f"   Days with Snapshots: {len(daily_balances)}")
            if len(daily_balances) >= 2:
                first_equity = daily_balances[0]['equity']
                last_equity = daily_balances[-1]['equity']
                print(f"   First Day Equity: ${first_equity:,.2f}")
                print(f"   Last Day Equity:  ${last_equity:,.2f}")

    # Optional CSV exporters
    def export_trades_csv(self, out_path: str | pathlib.Path) -> None:
        if not self.trades:
            return
        keys = sorted({k for e in self.trades for k in e.keys()})
        with open(out_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=keys)
            w.writeheader()
            for e in self.trades:
                w.writerow(e)

    def export_signals_csv(self, out_path: str | pathlib.Path) -> None:
        if not self.signals:
            return
        keys = sorted({k for e in self.signals for k in e.keys()})
        with open(out_path, "w", newline="", encoding="utf-8") as f:
            w = csv.DictWriter(f, fieldnames=keys)
            w.writeheader()
            for e in self.signals:
                w.writerow(e)

    def export_daily_summary(self, output_file: str):
        """Export daily summary to CSV"""
        daily_trades = self._analyze_daily_trades()
        daily_balances = self.get_daily_balance_changes()
        
        # Merge daily data
        daily_data = {}
        for d in daily_trades:
            daily_data[d['date']] = d
        for d in daily_balances:
            if d['date'] in daily_data:
                daily_data[d['date']].update(d)
            else:
                daily_data[d['date']] = d
        
        # Write CSV
        with open(output_file, 'w') as f:
            f.write("date,trades,volume,instruments,cash,equity,realized\n")
            for date in sorted(daily_data.keys()):
                data = daily_data[date]
                instruments = ','.join(data.get('instruments', []))
                f.write(f"{date},{data.get('trades', 0)},{data.get('volume', 0):.2f},"
                       f'"{instruments}",{data.get("cash", 0):.2f},'
                       f'{data.get("equity", 0):.2f},{data.get("realized", 0):.2f}\n')
        
        print(f"üìÑ Daily summary exported to: {output_file}")
    
    def all_events(self) -> List[Dict[str, Any]]:
        """Return all events in chronological order"""
        all_events = []
        all_events.extend(self.trades)
        all_events.extend(self.snapshots)
        all_events.extend(self.signals)
        for bars in self.bars.values():
            all_events.extend(bars)
        all_events.extend(self.other)
        
        # Sort by timestamp
        all_events.sort(key=lambda x: x.get('ts', 0))
        return all_events
```

## üìÑ **FILE 5 of 26**: tools/audit_chain_report.py

**File Information**:
- **Path**: `tools/audit_chain_report.py`

- **Size**: 65 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
from __future__ import annotations
import sys, json, pathlib
from collections import defaultdict

def load_events(path: str):
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                ev = json.loads(line)
                yield ev
            except Exception:
                continue

def main():
    if len(sys.argv) < 3:
        print("Usage: python tools/audit_chain_report.py <audit.jsonl> <out.txt>")
        sys.exit(1)
    audit_path = sys.argv[1]
    out_path = sys.argv[2]

    chains = defaultdict(lambda: {"signal": None, "routes": [], "orders": [], "fills": []})
    for ev in load_events(audit_path):
        t = ev.get("type", "")
        chain = ev.get("chain")
        if not chain:
            continue
        if t == "signal":
            chains[chain]["signal"] = ev
        elif t == "route":
            chains[chain]["routes"].append(ev)
        elif t == "order":
            chains[chain]["orders"].append(ev)
        elif t == "fill":
            chains[chain]["fills"].append(ev)

    lines = []
    lines.append(f"Audit Chain Report for: {audit_path}\n")
    keys = sorted(chains.keys(), key=lambda k: int(k.split(":")[0]))
    for k in keys:
        ch = chains[k]
        sig = ch["signal"] or {}
        ts = sig.get("ts", 0)
        base = sig.get("base", "")
        sig_code = sig.get("sig")
        conf = sig.get("conf")
        sig_name = {0:"BUY",1:"STRONG_BUY",2:"SELL",3:"STRONG_SELL",4:"HOLD"}.get(sig_code, str(sig_code))
        lines.append(f"ts={ts} chain={k} base={base} signal={sig_name} conf={conf}")
        for r in ch["routes"]:
            lines.append(f"  route: inst={r.get('inst')} tw={r.get('tw')}")
        for o in ch["orders"]:
            lines.append(f"  order: inst={o.get('inst')} side={o.get('side')} qty={o.get('qty')} limit={o.get('limit')}")
        for f in ch["fills"]:
            lines.append(f"  fill: inst={f.get('inst')} px={f.get('px')} qty={f.get('qty')} fees={f.get('fees')} side={f.get('side')} pnl_d={f.get('pnl_d')} eq_after={f.get('eq_after')} pos_after={f.get('pos_after')}")
        lines.append("")

    pathlib.Path(out_path).write_text("\n".join(lines), encoding='utf-8')
    print(f"Wrote {out_path}")

if __name__ == "__main__":
    main()



```

## üìÑ **FILE 6 of 26**: tools/audit_cli.py

**File Information**:
- **Path**: `tools/audit_cli.py`

- **Size**: 494 lines
- **Modified**: 2025-09-11 12:03:33

- **Type**: .py

```text
from __future__ import annotations
import argparse, pathlib, sys
from audit_analyzer import AuditAnalyzer

def print_usage():
    print("Usage: audit_cli <command> [options]")
    print("Commands:")
    print("  replay <audit_file.jsonl> [--summary] [--trades] [--metrics]")
    print("  format <audit_file.jsonl> [--output <output_file>] [--type <txt|csv>] [--trades-only] [--signals-only]")
    print("  trades <audit_file.jsonl> [--output <output_file>]")
    print("  analyze <audit_file.jsonl> [--trades-csv <file>] [--signals-csv <file>] [--daily-csv <file>] [--summary]")
    print("  latest [--max-trades <n>] [--audit-dir <dir>]")

def cmd_replay(args):
    """Replay command - shows basic audit information"""
    analyzer = AuditAnalyzer()
    analyzer.load(args.audit_file)
    s = analyzer.stats()
    
    # Default to showing everything if no specific flags
    show_summary = args.summary or (not args.trades and not args.metrics)
    show_trades = args.trades or (not args.summary and not args.metrics)
    show_metrics = args.metrics or (not args.summary and not args.trades)
    
    if show_summary:
        print("=== AUDIT REPLAY SUMMARY ===")
        print(f"Run ID: {analyzer.run_metadata.get('run', 'unknown')}")
        print(f"Strategy: {analyzer.run_metadata.get('strategy', 'unknown')}")
        print(f"Total Records: {sum(s.values())}")
        print(f"Trades: {s['trades']}")
        print(f"Snapshots: {s['snapshots']}")
        print(f"Signals: {s['signals']}")
        print()
    
    if show_metrics and analyzer.snapshots:
        initial = analyzer.snapshots[0]
        final = analyzer.snapshots[-1]
        
        initial_equity = initial.get('equity', 100000.0)
        final_equity = final.get('equity', 100000.0)
        total_return = (final_equity - initial_equity) / initial_equity
        monthly_return = (final_equity / initial_equity) ** (1.0/3.0) - 1.0
        
        print("=== PERFORMANCE METRICS ===")
        print(f"Initial Equity: ${initial_equity:.2f}")
        print(f"Final Equity: ${final_equity:.2f}")
        print(f"Total Return: {total_return:.4f} ({total_return*100:.2f}%)")
        print(f"Monthly Return: {monthly_return:.4f} ({monthly_return*100:.2f}%)")
        
        if analyzer.trades:
            print(f"Total Trades: {len(analyzer.trades)}")
            print(f"Avg Trades/Day: {len(analyzer.trades) / 63.0:.1f}")
            
            # Add instrument distribution analysis
            print("\n=== INSTRUMENT DISTRIBUTION ===")
            instrument_stats = {}
            total_trades = len(analyzer.trades)
            
            for trade in analyzer.trades:
                instrument = trade.get('inst', 'UNKNOWN')
                pnl = trade.get('pnl_d', 0.0)
                
                if instrument not in instrument_stats:
                    instrument_stats[instrument] = {
                        'count': 0,
                        'total_pnl': 0.0,
                        'winning_trades': 0,
                        'losing_trades': 0
                    }
                
                stats = instrument_stats[instrument]
                stats['count'] += 1
                stats['total_pnl'] += pnl
                
                if pnl > 0:
                    stats['winning_trades'] += 1
                elif pnl < 0:
                    stats['losing_trades'] += 1
            
            # Sort by trade count (most active first)
            sorted_instruments = sorted(instrument_stats.items(), key=lambda x: x[1]['count'], reverse=True)
            
            for instrument, stats in sorted_instruments:
                percentage = (stats['count'] / total_trades * 100) if total_trades > 0 else 0
                win_rate = (stats['winning_trades'] / stats['count'] * 100) if stats['count'] > 0 else 0
                
                print(f"{instrument:>6}: {stats['count']:>4} trades ({percentage:>5.1f}%) | "
                      f"P&L: ${stats['total_pnl']:>8.2f} | Win Rate: {win_rate:>5.1f}%")
        print()
    
    if show_trades and analyzer.trades:
        print("=== RECENT TRADES (Last 10) ===")
        print("Time                Side Instr   Quantity    Price      PnL")
        print("----------------------------------------------------------------")
        
        start_idx = max(0, len(analyzer.trades) - 10)
        for trade in analyzer.trades[start_idx:]:
            side = trade.get('side', '')
            inst = trade.get('inst', '')
            qty = trade.get('qty', 0.0)
            price = trade.get('price', 0.0)
            pnl = trade.get('pnl', 0.0)
            ts = trade.get('ts', 0)
            
            # Convert timestamp
            from datetime import datetime
            dt = datetime.fromtimestamp(ts)
            time_str = dt.strftime("%Y-%m-%d %H:%M")
            
            print(f"{time_str:<19} {side:<5} {inst:<7} {qty:<10.0f} {price:<10.2f} {pnl:<10.2f}")

def cmd_format(args):
    """Format command - converts audit file to human-readable or CSV format"""
    analyzer = AuditAnalyzer()
    analyzer.load(args.audit_file)
    
    output_file = args.output
    if not output_file:
        # Generate default output filename
        base_name = pathlib.Path(args.audit_file).stem
        if args.trades_only:
            output_file = f"audit/{base_name}_trades_only.txt"
        elif args.signals_only:
            output_file = f"audit/{base_name}_signals_only.txt"
        elif args.type == "csv":
            output_file = f"audit/{base_name}_data.csv"
        else:
            output_file = f"audit/{base_name}_human_readable.txt"
    
    # Create output directory if needed
    pathlib.Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w') as f:
        if args.type == "csv":
            f.write("Timestamp,Type,Symbol,Side,Quantity,Price,Trade_PnL,Cash,Realized_PnL,Unrealized_PnL,Total_Equity\n")
        elif args.trades_only:
            f.write("TRADES ONLY - AUDIT LOG\n")
            f.write("=======================\n\n")
            f.write("Format: [#] TIMESTAMP | TICKER | BUY/SELL | QUANTITY @ PRICE | EQUITY_AFTER\n")
            f.write("---------------------------------------------------------------------------------\n\n")
        elif args.signals_only:
            f.write("SIGNAL-TO-TRADE PIPELINE - AUDIT LOG\n")
            f.write("=====================================\n\n")
            f.write("Format: Signal (p) ‚Üí Router ‚Üí Sizer ‚Üí Runner ‚Üí Balance Changes\n")
            f.write("-------------------------------------------------------------\n\n")
        else:
            f.write("HUMAN-READABLE AUDIT LOG\n")
            f.write("========================\n\n")
        
        if args.signals_only:
            # Group events by chain for signals-only mode
            chain_events = {}
            for event in analyzer.all_events():
                chain_id = event.get('chain', 'no_chain')
                if chain_id not in chain_events:
                    chain_events[chain_id] = []
                chain_events[chain_id].append(event)
            
            # Sort chains by timestamp
            sorted_chains = sorted(chain_events.items(), key=lambda x: x[1][0].get('ts', 0) if x[1] else 0)
            
            line_num = 0
            for chain_id, events in sorted_chains:
                # Sort events within chain by sequence
                events.sort(key=lambda x: x.get('seq', 0))
                
                signal_event = None
                orders = []
                fills = []
                
                # Categorize events
                for event in events:
                    event_type = event.get('type', '')
                    if event_type == 'signal':
                        signal_event = event
                    elif event_type == 'order':
                        orders.append(event)
                    elif event_type == 'fill':
                        fills.append(event)
                
                if not signal_event:
                    continue
                
                line_num += 1
                
                # Convert timestamp
                from datetime import datetime
                ts = signal_event.get('ts', 0)
                dt = datetime.fromtimestamp(ts)
                time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                
                # Format signal
                prob = signal_event.get('p', signal_event.get('conf', 0.0))
                sig_type = signal_event.get('sig', 4)
                sig_names = {0: "BUY", 1: "STRONG_BUY", 2: "SELL", 3: "STRONG_SELL", 4: "HOLD"}
                sig_name = sig_names.get(sig_type, f"SIG_{sig_type}")
                base_symbol = signal_event.get('base', '')
                
                f.write(f"[{line_num:3d}] {time_str} SIGNAL: {base_symbol} {sig_name} (p={prob:.3f})\n")
                
                # Show router decisions (orders)
                if orders:
                    f.write(f"      ROUTER: ")
                    router_decisions = []
                    for order in orders:
                        inst = order.get('inst', '')
                        side = "BUY" if order.get('side', 0) == 0 else "SELL"
                        qty = order.get('qty', 0.0)
                        router_decisions.append(f"{side} {qty:.2f} {inst}")
                    f.write(" ‚Üí ".join(router_decisions) + "\n")
                
                # Show sizer/runner decisions (fills)
                if fills:
                    f.write(f"      RUNNER: ")
                    runner_decisions = []
                    total_pnl = 0.0
                    for fill in fills:
                        inst = fill.get('inst', '')
                        side = "BUY" if fill.get('side', 0) == 0 else "SELL"
                        qty = fill.get('qty', 0.0)
                        px = fill.get('px', 0.0)
                        pnl = fill.get('pnl_d', 0.0)
                        total_pnl += pnl
                        runner_decisions.append(f"{side} {qty:.2f} {inst} @ ${px:.2f}")
                    f.write(" ‚Üí ".join(runner_decisions) + f" | P&L: ${total_pnl:.2f}\n")
                
                f.write("\n")
        else:
            # Regular processing for non-signals-only mode
            line_num = 0
            for event in analyzer.all_events():
                line_num += 1
                event_type = event.get('type', '')
                ts = event.get('ts', 0)
                
                # Convert timestamp
                from datetime import datetime
                dt = datetime.fromtimestamp(ts)
                time_str = dt.strftime("%Y-%m-%d %H:%M:%S")
                
                if args.type == "csv":
                    f.write(f"{time_str},{event_type}")
                    if event_type == "fill":
                        side_str = "BUY" if event.get('side', 0) == 0 else "SELL"
                        f.write(f",{event.get('inst', '')},{side_str},{event.get('qty', 0.0)},{event.get('px', 0.0)},{event.get('pnl_d', 0.0)},,,")
                    elif event_type == "snapshot":
                        f.write(f",,,,,,{event.get('cash', 0.0)},{event.get('real', 0.0)},{event.get('equity', 0.0) - event.get('cash', 0.0) - event.get('real', 0.0)},{event.get('equity', 0.0)}")
                    else:
                        f.write(",,,,,,,,")
                    f.write("\n")
                elif args.trades_only:
                    if event_type == "fill":
                        side_str = "BUY" if event.get('side', 0) == 0 else "SELL"
                        f.write(f"[{line_num:4d}] {time_str} | {event.get('inst', ''):<5} | {side_str:<4} | {event.get('qty', 0.0):<8.0f} @ ${event.get('px', 0.0):<8.2f} | Equity: ${event.get('eq_after', 0.0)}\n")
                else:
                    f.write(f"[{line_num:4d}] {time_str} ")
                    if event_type == "run_start":
                        meta = event.get('meta', {})
                        f.write(f"RUN START - Strategy: {meta.get('strategy', '')}, Series: {meta.get('total_series', 0)}\n")
                    elif event_type == "fill":
                        side_str = "BUY" if event.get('side', 0) == 0 else "SELL"
                        f.write(f"TRADE - {side_str} {event.get('qty', 0.0)} {event.get('inst', '')} @ ${event.get('px', 0.0):.2f} (P&L: ${event.get('pnl_d', 0.0)})\n")
                    elif event_type == "snapshot":
                        cash = event.get('cash', 0.0)
                        equity = event.get('equity', 0.0)
                        realized = event.get('real', 0.0)
                        unrealized = equity - cash - realized
                        f.write(f"PORTFOLIO - Cash: ${cash:.2f}, Realized P&L: ${realized}, Unrealized P&L: ${unrealized}, Total Equity: ${equity}\n")
                    elif event_type == "signal":
                        # Use 'p' as the probability value (updated audit system now stores 'p' instead of 'conf')
                        prob = event.get('p', event.get('conf', 0.0))  # Support both old 'conf' and new 'p' fields
                        sig_type = event.get('sig', 4)  # 4=HOLD, 0=BUY, 1=STRONG_BUY, 2=SELL, 3=STRONG_SELL
                        sig_names = {0: "BUY", 1: "STRONG_BUY", 2: "SELL", 3: "STRONG_SELL", 4: "HOLD"}
                        sig_name = sig_names.get(sig_type, f"SIG_{sig_type}")
                        base_symbol = event.get('base', '')
                        f.write(f"SIGNAL - {base_symbol} {sig_name} p={prob:.3f}\n")
                    elif event_type == "bar":
                        f.write(f"BAR - {event.get('inst', '')} O:{event.get('o', 0.0):.2f} H:{event.get('h', 0.0):.2f} L:{event.get('l', 0.0):.2f} C:{event.get('c', 0.0):.2f} V:{event.get('v', 0.0):.0f}\n")
                    else:
                        f.write(f"{event_type} - {event}\n")
    
    print(f"Formatted audit log written to: {output_file}")

def cmd_trades(args):
    """Trades command - shows only trades"""
    args.type = "txt"
    args.trades_only = True
    cmd_format(args)

def cmd_analyze(args):
    """Analyze command - comprehensive analysis with CSV export"""
    analyzer = AuditAnalyzer()
    analyzer.load(args.audit_file)
    s = analyzer.stats()
    print(f"‚úÖ Loaded: trades={s['trades']} snapshots={s['snapshots']} signals={s['signals']} bars={s['bars']} other={s['other']}")

    if args.summary:
        analyzer.print_summary()

    if args.trades_csv:
        analyzer.export_trades_csv(args.trades_csv)
        print(f"üíæ Wrote trades CSV: {args.trades_csv}")
    if args.signals_csv:
        analyzer.export_signals_csv(args.signals_csv)
        print(f"üíæ Wrote signals CSV: {args.signals_csv}")
    if args.daily_csv:
        analyzer.export_daily_summary(args.daily_csv)
        print(f"üíæ Wrote daily summary CSV: {args.daily_csv}")

def cmd_latest(args):
    """Latest command - automatically find latest audit file and show quick metrics"""
    import glob
    import os
    
    # Find audit directory
    audit_dir = args.audit_dir if args.audit_dir else "audit"
    if not os.path.exists(audit_dir):
        print(f"‚ùå Audit directory '{audit_dir}' not found")
        return 1
    
    # Find all .jsonl files
    pattern = os.path.join(audit_dir, "*.jsonl")
    audit_files = glob.glob(pattern)
    
    if not audit_files:
        print(f"‚ùå No audit files found in '{audit_dir}'")
        return 1
    
    # Sort by modification time (newest first)
    audit_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    latest_file = audit_files[0]
    
    print(f"üîç Found latest audit file: {os.path.basename(latest_file)}")
    print(f"üìÖ Modified: {pathlib.Path(latest_file).stat().st_mtime}")
    print()
    
    # Load and analyze
    analyzer = AuditAnalyzer()
    analyzer.load(latest_file)
    s = analyzer.stats()
    
    # Quick summary
    print("=== QUICK METRICS ===")
    print(f"Strategy: {analyzer.run_metadata.get('strategy', 'Unknown')}")
    print(f"Total Records: {sum(s.values())}")
    print(f"Trades: {s['trades']}")
    print(f"Snapshots: {s['snapshots']}")
    print(f"Signals: {s['signals']}")
    print()
    
    # Performance metrics
    if analyzer.snapshots:
        initial = analyzer.snapshots[0]
        final = analyzer.snapshots[-1]
        
        initial_equity = initial.get('equity', 100000.0)
        final_equity = final.get('equity', 100000.0)
        total_return = (final_equity - initial_equity) / initial_equity
        
        print("=== PERFORMANCE ===")
        print(f"Initial Equity: ${initial_equity:,.2f}")
        print(f"Final Equity:   ${final_equity:,.2f}")
        print(f"Total Return:   {total_return:.4f} ({total_return*100:+.2f}%)")
        
        if analyzer.trades:
            print(f"Total Trades:   {len(analyzer.trades)}")
            print(f"Avg Trades/Day: {len(analyzer.trades) / 63.0:.1f}")
            
            # Add instrument distribution analysis
            print("\n=== INSTRUMENT DISTRIBUTION ===")
            instrument_stats = {}
            total_trades = len(analyzer.trades)
            
            for trade in analyzer.trades:
                instrument = trade.get('inst', 'UNKNOWN')
                pnl = trade.get('pnl_d', 0.0)
                
                if instrument not in instrument_stats:
                    instrument_stats[instrument] = {
                        'count': 0,
                        'total_pnl': 0.0,
                        'winning_trades': 0,
                        'losing_trades': 0
                    }
                
                stats = instrument_stats[instrument]
                stats['count'] += 1
                stats['total_pnl'] += pnl
                
                if pnl > 0:
                    stats['winning_trades'] += 1
                elif pnl < 0:
                    stats['losing_trades'] += 1
            
            # Sort by trade count (most active first)
            sorted_instruments = sorted(instrument_stats.items(), key=lambda x: x[1]['count'], reverse=True)
            
            for instrument, stats in sorted_instruments:
                percentage = (stats['count'] / total_trades * 100) if total_trades > 0 else 0
                win_rate = (stats['winning_trades'] / stats['count'] * 100) if stats['count'] > 0 else 0
                
                print(f"{instrument:>6}: {stats['count']:>4} trades ({percentage:>5.1f}%) | "
                      f"P&L: ${stats['total_pnl']:>8.2f} | Win Rate: {win_rate:>5.1f}%")
        print()
    
    # Recent trades
    if analyzer.trades:
        max_trades = args.max_trades
        recent_trades = analyzer.trades[-max_trades:] if len(analyzer.trades) > max_trades else analyzer.trades
        
        print(f"=== RECENT TRADES (Last {len(recent_trades)}) ===")
        print("Time                Side Instr   Quantity    Price      PnL")
        print("----------------------------------------------------------------")
        
        for trade in recent_trades:
            side = trade.get('side', '')
            inst = trade.get('inst', '')
            qty = trade.get('qty', 0.0)
            price = trade.get('price', 0.0)
            pnl = trade.get('pnl', 0.0)
            ts = trade.get('ts', 0)
            
            # Convert timestamp
            from datetime import datetime
            dt = datetime.fromtimestamp(ts)
            time_str = dt.strftime("%Y-%m-%d %H:%M")
            
            print(f"{time_str:<19} {side:<5} {inst:<7} {qty:<10.0f} {price:<10.2f} {pnl:<10.2f}")
    else:
        print("=== RECENT TRADES ===")
        print("No trades found in this audit file.")
    
    return 0

def main():
    if len(sys.argv) < 2:
        print_usage()
        return 1
    
    command = sys.argv[1]
    
    if command == "replay":
        parser = argparse.ArgumentParser(description="Replay audit file")
        parser.add_argument("audit_file", help="Path to audit file (.jsonl)")
        parser.add_argument("--summary", action="store_true", help="Show summary")
        parser.add_argument("--trades", action="store_true", help="Show trades")
        parser.add_argument("--metrics", action="store_true", help="Show metrics")
        args = parser.parse_args(sys.argv[2:])
        cmd_replay(args)
        
    elif command == "format":
        parser = argparse.ArgumentParser(description="Format audit file")
        parser.add_argument("audit_file", help="Path to audit file (.jsonl)")
        parser.add_argument("--output", help="Output file path")
        parser.add_argument("--type", choices=["txt", "csv"], default="txt", help="Output format")
        parser.add_argument("--trades-only", action="store_true", help="Show only trades")
        parser.add_argument("--signals-only", action="store_true", help="Show only signal-to-trade pipeline")
        args = parser.parse_args(sys.argv[2:])
        cmd_format(args)
        
    elif command == "trades":
        parser = argparse.ArgumentParser(description="Show trades only")
        parser.add_argument("audit_file", help="Path to audit file (.jsonl)")
        parser.add_argument("--output", help="Output file path")
        args = parser.parse_args(sys.argv[2:])
        args.format_type = "txt"
        args.trades_only = True
        cmd_trades(args)
        
    elif command == "analyze":
        parser = argparse.ArgumentParser(description="Comprehensive audit analysis")
        parser.add_argument("audit_file", help="Path to audit file (.jsonl)")
        parser.add_argument("--trades-csv", help="Export trades CSV")
        parser.add_argument("--signals-csv", help="Export signals CSV")
        parser.add_argument("--daily-csv", help="Export daily summary CSV")
        parser.add_argument("--summary", action="store_true", help="Print detailed summary")
        args = parser.parse_args(sys.argv[2:])
        cmd_analyze(args)
        
    elif command == "latest":
        parser = argparse.ArgumentParser(description="Show latest audit file metrics")
        parser.add_argument("--max-trades", type=int, default=20, help="Maximum number of recent trades to show (default: 20)")
        parser.add_argument("--audit-dir", default="audit", help="Audit directory to search (default: audit)")
        args = parser.parse_args(sys.argv[2:])
        return cmd_latest(args)
        
    else:
        print_usage()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())

```

## üìÑ **FILE 7 of 26**: tools/audit_parser.py

**File Information**:
- **Path**: `tools/audit_parser.py`

- **Size**: 91 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
from __future__ import annotations
import io, gzip, json, sys, pathlib
from typing import Iterator, Tuple, Optional, Dict, Any

class LineParseError(Exception):
    pass

def _iter_json_objects_from_string(s: str) -> Iterator[Dict[str, Any]]:
    """
    Robustly parse one or more JSON objects concatenated in a single string.
    Example:
      {"a":1}{"b":2}
      {"event":...},{"sha1":"..."}
    """
    dec = json.JSONDecoder()
    i, n = 0, len(s)
    while i < n:
        # Skip whitespace and stray commas
        while i < n and s[i] in " \t\r\n,":
            i += 1
        if i >= n: 
            break
        obj, end = dec.raw_decode(s, i)
        yield obj
        i = end

def parse_audit_line(line: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """
    Returns (event_obj, sha1_hex) for a single audit line.
    If the line has only an event, sha1_hex is None.
    If malformed, returns (None, None) and lets caller decide logging policy.
    """
    try:
        # Handle the specific format: {"main_object"},"sha1":"hash_value"}
        if '},"sha1":"' in line and line.endswith('}'):
            # Split at the SHA1 separator
            parts = line.split('},"sha1":"', 1)
            if len(parts) == 2:
                json_part = parts[0] + '}'
                sha1_part = parts[1].rstrip('}"')
                
                # Parse the main JSON object
                event = json.loads(json_part)
                sha1 = sha1_part
                return event, sha1
        
        # Fallback: try to parse as multiple JSON objects
        objs = list(_iter_json_objects_from_string(line))
        if not objs:
            return None, None
        # Strategy:
        # - If there's a dict with a 'type' (event) and a dict with only 'sha1', pair them.
        # - If multiple events in same line (unexpected), use the first, prefer final sha1.
        event = None
        sha1 = None
        for obj in objs:
            if isinstance(obj, dict) and "sha1" in obj and len(obj) == 1:
                # Trailing checksum
                sha1 = obj.get("sha1")
            elif isinstance(obj, dict) and obj.get("type"):
                # Candidate event
                if event is None:
                    event = obj
        return event, sha1
    except json.JSONDecodeError:
        return None, None

def open_maybe_gz(path: str | pathlib.Path) -> io.TextIOBase:
    p = pathlib.Path(path)
    if p.suffix == ".gz":
        return io.TextIOWrapper(gzip.open(p, "rb"), encoding="utf-8", newline="")
    return open(p, "r", encoding="utf-8", newline="")

def iter_audit_file(path: str | pathlib.Path, *, max_json_errors: int = 10) -> Iterator[Tuple[int, Dict[str, Any], Optional[str]]]:
    """
    Yields (line_num, event, sha1) for each well-formed event line.
    Silently skips lines that cannot be parsed as JSON (up to max_json_errors logged to stderr).
    """
    json_errs = 0
    with open_maybe_gz(path) as f:
        for ln, raw in enumerate(f, start=1):
            line = raw.strip()
            if not line:
                continue
            event, sha1 = parse_audit_line(line)
            if event is None:
                if json_errs < max_json_errors:
                    json_errs += 1
                    print(f"‚ö†Ô∏è  Invalid JSON on line {ln}: {line[:140]}...", file=sys.stderr)
                continue
            yield ln, event, sha1

```

## üìÑ **FILE 8 of 26**: tools/compare_real_vs_virtual.cpp

**File Information**:
- **Path**: `tools/compare_real_vs_virtual.cpp`

- **Size**: 220 lines
- **Modified**: 2025-09-11 02:45:28

- **Type**: .cpp

```text
#include "sentio/virtual_market.hpp"
#include <iostream>
#include <iomanip>
#include <fstream>
#include <sstream>
#include <vector>
#include <string>

struct BarData {
    std::string timestamp;
    double open, high, low, close;
    double volume;
    double return_pct;
};

std::vector<BarData> load_real_qqq_data(const std::string& filename, int count) {
    std::vector<BarData> bars;
    std::ifstream file(filename);
    std::string line;
    
    // Read all lines first
    std::vector<std::string> all_lines;
    while (std::getline(file, line)) {
        all_lines.push_back(line);
    }
    
    // Take the last 'count' lines
    int start_idx = std::max(0, (int)all_lines.size() - count);
    for (int i = start_idx; i < all_lines.size(); ++i) {
        std::istringstream iss(all_lines[i]);
        std::string timestamp_str, epoch_str, open_str, high_str, low_str, close_str, volume_str;
        
        if (std::getline(iss, timestamp_str, ',') &&
            std::getline(iss, epoch_str, ',') &&
            std::getline(iss, open_str, ',') &&
            std::getline(iss, high_str, ',') &&
            std::getline(iss, low_str, ',') &&
            std::getline(iss, close_str, ',') &&
            std::getline(iss, volume_str)) {
            
            BarData bar;
            bar.timestamp = timestamp_str;
            bar.open = std::stod(open_str);
            bar.high = std::stod(high_str);
            bar.low = std::stod(low_str);
            bar.close = std::stod(close_str);
            bar.volume = std::stod(volume_str);
            bar.return_pct = 0.0; // Will calculate later
            
            bars.push_back(bar);
        }
    }
    
    // Calculate returns
    for (size_t i = 1; i < bars.size(); ++i) {
        bars[i].return_pct = ((bars[i].close - bars[i-1].close) / bars[i-1].close) * 100.0;
    }
    
    return bars;
}

std::vector<BarData> generate_virtual_qqq_data(int count) {
    sentio::VirtualMarketEngine vm_engine;
    std::vector<sentio::Bar> vm_bars = vm_engine.generate_market_data("QQQ", count, 60);
    
    std::vector<BarData> bars;
    double prev_close = 0.0;
    
    for (size_t i = 0; i < vm_bars.size(); ++i) {
        const auto& vm_bar = vm_bars[i];
        
        BarData bar;
        bar.timestamp = std::to_string(vm_bar.ts_utc_epoch);
        bar.open = vm_bar.open;
        bar.high = vm_bar.high;
        bar.low = vm_bar.low;
        bar.close = vm_bar.close;
        bar.volume = vm_bar.volume;
        
        if (i > 0 && prev_close > 0) {
            bar.return_pct = ((bar.close - prev_close) / prev_close) * 100.0;
        } else {
            bar.return_pct = 0.0;
        }
        
        bars.push_back(bar);
        prev_close = bar.close;
    }
    
    return bars;
}

void print_comparison(const std::vector<BarData>& real_bars, const std::vector<BarData>& virtual_bars) {
    std::cout << "REAL QQQ vs VIRTUAL QQQ - Last 20 Minutes Comparison" << std::endl;
    std::cout << "================================================================================================================" << std::endl;
    std::cout << std::setw(4) << "Bar" 
              << std::setw(20) << "Real Timestamp" 
              << std::setw(8) << "Real" 
              << std::setw(8) << "Real%" 
              << std::setw(8) << "RealVol"
              << std::setw(20) << "Virtual Timestamp" 
              << std::setw(8) << "Virtual" 
              << std::setw(8) << "Virtual%" 
              << std::setw(8) << "VirtualVol" << std::endl;
    std::cout << "================================================================================================================" << std::endl;
    
    size_t max_bars = std::max(real_bars.size(), virtual_bars.size());
    
    for (size_t i = 0; i < max_bars; ++i) {
        std::cout << std::setw(4) << (i + 1);
        
        // Real data
        if (i < real_bars.size()) {
            const auto& real = real_bars[i];
            std::cout << std::setw(20) << real.timestamp.substr(11, 8) // Show only time part
                      << std::setw(8) << std::fixed << std::setprecision(2) << real.close
                      << std::setw(8) << std::fixed << std::setprecision(3) << real.return_pct
                      << std::setw(8) << std::fixed << std::setprecision(0) << real.volume;
        } else {
            std::cout << std::setw(20) << "N/A"
                      << std::setw(8) << "N/A"
                      << std::setw(8) << "N/A"
                      << std::setw(8) << "N/A";
        }
        
        // Virtual data
        if (i < virtual_bars.size()) {
            const auto& virtual_bar = virtual_bars[i];
            std::cout << std::setw(20) << virtual_bar.timestamp
                      << std::setw(8) << std::fixed << std::setprecision(2) << virtual_bar.close
                      << std::setw(8) << std::fixed << std::setprecision(3) << virtual_bar.return_pct
                      << std::setw(8) << std::fixed << std::setprecision(0) << virtual_bar.volume;
        } else {
            std::cout << std::setw(20) << "N/A"
                      << std::setw(8) << "N/A"
                      << std::setw(8) << "N/A"
                      << std::setw(8) << "N/A";
        }
        
        std::cout << std::endl;
    }
    
    // Calculate statistics
    std::cout << "\n================================================================================================================" << std::endl;
    std::cout << "STATISTICS COMPARISON" << std::endl;
    std::cout << "================================================================================================================" << std::endl;
    
    if (!real_bars.empty() && !virtual_bars.empty()) {
        // Real data stats
        double real_total_return = ((real_bars.back().close - real_bars.front().open) / real_bars.front().open) * 100.0;
        double real_sum_returns = 0.0, real_sum_squared = 0.0;
        double real_avg_volume = 0.0;
        
        for (const auto& bar : real_bars) {
            real_sum_returns += bar.return_pct;
            real_sum_squared += bar.return_pct * bar.return_pct;
            real_avg_volume += bar.volume;
        }
        
        double real_mean_return = real_sum_returns / real_bars.size();
        double real_variance = (real_sum_squared / real_bars.size()) - (real_mean_return * real_mean_return);
        double real_volatility = std::sqrt(real_variance);
        real_avg_volume /= real_bars.size();
        
        // Virtual data stats
        double virtual_total_return = ((virtual_bars.back().close - virtual_bars.front().open) / virtual_bars.front().open) * 100.0;
        double virtual_sum_returns = 0.0, virtual_sum_squared = 0.0;
        double virtual_avg_volume = 0.0;
        
        for (const auto& bar : virtual_bars) {
            virtual_sum_returns += bar.return_pct;
            virtual_sum_squared += bar.return_pct * bar.return_pct;
            virtual_avg_volume += bar.volume;
        }
        
        double virtual_mean_return = virtual_sum_returns / virtual_bars.size();
        double virtual_variance = (virtual_sum_squared / virtual_bars.size()) - (virtual_mean_return * virtual_mean_return);
        double virtual_volatility = std::sqrt(virtual_variance);
        virtual_avg_volume /= virtual_bars.size();
        
        std::cout << std::setw(20) << "Metric" 
                  << std::setw(15) << "Real QQQ" 
                  << std::setw(15) << "Virtual QQQ" 
                  << std::setw(15) << "Difference" << std::endl;
        std::cout << "----------------------------------------------------------------------------------------------------------------" << std::endl;
        
        std::cout << std::setw(20) << "Total Return %" 
                  << std::setw(15) << std::fixed << std::setprecision(3) << real_total_return
                  << std::setw(15) << std::fixed << std::setprecision(3) << virtual_total_return
                  << std::setw(15) << std::fixed << std::setprecision(3) << (virtual_total_return - real_total_return) << std::endl;
        
        std::cout << std::setw(20) << "Volatility %" 
                  << std::setw(15) << std::fixed << std::setprecision(3) << real_volatility
                  << std::setw(15) << std::fixed << std::setprecision(3) << virtual_volatility
                  << std::setw(15) << std::fixed << std::setprecision(3) << (virtual_volatility - real_volatility) << std::endl;
        
        std::cout << std::setw(20) << "Avg Volume" 
                  << std::setw(15) << std::fixed << std::setprecision(0) << real_avg_volume
                  << std::setw(15) << std::fixed << std::setprecision(0) << virtual_avg_volume
                  << std::setw(15) << std::fixed << std::setprecision(0) << (virtual_avg_volume - real_avg_volume) << std::endl;
        
        std::cout << std::setw(20) << "Price Range" 
                  << std::setw(15) << std::fixed << std::setprecision(2) << (real_bars.back().close - real_bars.front().open)
                  << std::setw(15) << std::fixed << std::setprecision(2) << (virtual_bars.back().close - virtual_bars.front().open)
                  << std::setw(15) << std::fixed << std::setprecision(2) << ((virtual_bars.back().close - virtual_bars.front().open) - (real_bars.back().close - real_bars.front().open)) << std::endl;
    }
}

int main() {
    // Load last 20 minutes of real QQQ data
    std::vector<BarData> real_bars = load_real_qqq_data("data/equities/QQQ_RTH_NH.csv", 20);
    
    // Generate 20 minutes of virtual QQQ data
    std::vector<BarData> virtual_bars = generate_virtual_qqq_data(20);
    
    // Print comparison
    print_comparison(real_bars, virtual_bars);
    
    return 0;
}

```

## üìÑ **FILE 9 of 26**: tools/create_mega_document.py

**File Information**:
- **Path**: `tools/create_mega_document.py`

- **Size**: 104 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
"""
Create mega document from source files.
"""

import os
import argparse
import datetime
from pathlib import Path

def create_mega_document(directories, title, description, output, include_bug_report=False, bug_report_file=None):
    """Create a mega document from source files."""
    
    print(f"üîß Creating mega document: {output}")
    print(f"üìÅ Source directory: {os.getcwd()}")
    print(f"üìÅ Output file: {output}")
    
    # Collect all files
    all_files = []
    for directory in directories:
        if os.path.exists(directory):
            for root, dirs, files in os.walk(directory):
                for file in files:
                    if file.endswith(('.hpp', '.cpp', '.h', '.c', '.py', '.md', '.txt', '.cmake', 'CMakeLists.txt')):
                        file_path = os.path.join(root, file)
                        all_files.append(file_path)
    
    print(f"üìÅ Files to include: {len(all_files)}")
    
    # Sort files for consistent ordering
    all_files.sort()
    
    # Create mega document
    with open(output, 'w', encoding='utf-8') as f:
        f.write(f"# {title}\n\n")
        f.write(f"**Generated**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**Source Directory**: {os.getcwd()}\n")
        f.write(f"**Description**: {description}\n\n")
        f.write(f"**Total Files**: {len(all_files)}\n\n")
        f.write("---\n\n")
        
        # Include bug report if requested
        if include_bug_report and bug_report_file and os.path.exists(bug_report_file):
            f.write("## üêõ **BUG REPORT**\n\n")
            with open(bug_report_file, 'r', encoding='utf-8') as bug_f:
                f.write(bug_f.read())
            f.write("\n\n---\n\n")
        
        # Table of contents
        f.write("## üìã **TABLE OF CONTENTS**\n\n")
        for i, file_path in enumerate(all_files, 1):
            f.write(f"{i}. [{file_path}](#file-{i})\n")
        f.write("\n---\n\n")
        
        # File contents
        for i, file_path in enumerate(all_files, 1):
            try:
                with open(file_path, 'r', encoding='utf-8') as file_f:
                    content = file_f.read()
                
                f.write(f"## üìÑ **FILE {i} of {len(all_files)}**: {file_path}\n\n")
                f.write("**File Information**:\n")
                f.write(f"- **Path**: `{file_path}`\n\n")
                f.write(f"- **Size**: {len(content.splitlines())} lines\n")
                f.write(f"- **Modified**: {datetime.datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"- **Type**: {Path(file_path).suffix}\n\n")
                f.write("```text\n")
                f.write(content)
                f.write("\n```\n\n")
                
                print(f"üìÑ Processing file {i}/{len(all_files)}: {file_path}")
                
            except Exception as e:
                print(f"‚ùå Error processing {file_path}: {e}")
                f.write(f"## üìÑ **FILE {i} of {len(all_files)}**: {file_path}\n\n")
                f.write(f"**Error**: Could not read file - {e}\n\n")
    
    print(f"‚úÖ Mega document created: {output}")
    print(f"üìä Output size: {os.path.getsize(output) / 1024:.1f} KB")
    print(f"üìä Files processed: {len(all_files)}/{len(all_files)}")
    print(f"üìä Content size: {sum(os.path.getsize(f) for f in all_files if os.path.exists(f)) / 1024:.1f} KB")
    print(f"\nüéØ Success! Mega document created:")
    print(f"{output}")
    print(f"\nüìÅ Location: {os.path.abspath(output)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Create mega document from source files")
    parser.add_argument("--directories", "-d", nargs="+", required=True, help="Directories to include")
    parser.add_argument("--title", "-t", required=True, help="Document title")
    parser.add_argument("--description", "-desc", required=True, help="Document description")
    parser.add_argument("--output", "-o", required=True, help="Output file path")
    parser.add_argument("--include-bug-report", action="store_true", help="Include bug report")
    parser.add_argument("--bug-report-file", help="Bug report file path")
    
    args = parser.parse_args()
    
    create_mega_document(
        args.directories,
        args.title,
        args.description,
        args.output,
        args.include_bug_report,
        args.bug_report_file
    )

```

## üìÑ **FILE 10 of 26**: tools/csv_runner.cpp

**File Information**:
- **Path**: `tools/csv_runner.cpp`

- **Size**: 73 lines
- **Modified**: 2025-09-11 12:27:04

- **Type**: .cpp

```text
#include <iostream>
#include <fstream>
#include <sstream>
#include <unordered_map>
#include "sentio/rsi_strategy.hpp"

using namespace sentio;

static bool parse_csv_row(const std::string& line, Bar& b, bool& header_checked) {
    if (!header_checked) {
        header_checked = true;
        if (!line.empty() && !std::isdigit(line[0])) return false; // skip header
    }
    std::stringstream ss(line);
    std::string cell; std::vector<std::string> cols;
    while (std::getline(ss, cell, ',')) cols.push_back(cell);
    if (cols.size() < 6) return false;
    b.ts_utc_epoch = std::stoll(cols[0]);
    b.open   = std::stod(cols[1]);
    b.high   = std::stod(cols[2]);
    b.low    = std::stod(cols[3]);
    b.close  = std::stod(cols[4]);
    b.volume = std::stoull(cols[5]);
    return true;
}

int main(int argc, char** argv) {
    if (argc < 2) { std::cerr << "Usage: csv_runner <prices.csv>\n"; return 1; }
    std::ifstream f(argv[1]);
    if (!f) { std::cerr << "Cannot open " << argv[1] << "\n"; return 2; }

    RSIStrategy strat;
    strat.set_params({
        {"rsi_period", 14},
        {"epsilon", 0.05},
        {"weight_clip", 1.0},
        {"alpha", 1.0}   // >1 steeper; <1 flatter
    });
    strat.apply_params();

    std::vector<Bar> bars;
    bool header=false;
    std::string line;
    
    // Load all bars first
    while (std::getline(f, line)) {
        Bar b;
        if (!parse_csv_row(line, b, header)) continue;
        bars.push_back(b);
    }
    
    std::cout << "Loaded " << bars.size() << " bars\n";
    
    // Test the strategy
    long n_signals = 0;
    double sum_prob = 0.0;
    double min_prob = 1.0, max_prob = 0.0;
    
    for (int i = 0; i < static_cast<int>(bars.size()); ++i) {
        double prob = strat.calculate_probability(bars, i);
        if (prob != 0.5) { // Not neutral
            ++n_signals;
            sum_prob += prob;
            min_prob = std::min(min_prob, prob);
            max_prob = std::max(max_prob, prob);
        }
    }
    
    std::cout << "Signals generated: " << n_signals << "\n";
    std::cout << "Avg probability: " << (n_signals ? sum_prob/n_signals : 0.0) << "\n";
    std::cout << "Probability range: [" << min_prob << ", " << max_prob << "]\n";
    return 0;
}

```

## üìÑ **FILE 11 of 26**: tools/data_downloader.py

**File Information**:
- **Path**: `tools/data_downloader.py`

- **Size**: 205 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
import os
import argparse
import requests
import pandas as pd
import pandas_market_calendars as mcal
import struct
from datetime import datetime
from pathlib import Path

# --- Constants ---
# Define the Regular Trading Hours for NYSE in New York time.
RTH_START = "09:30"
RTH_END = "16:00"
NY_TIMEZONE = "America/New_York"
POLYGON_API_BASE = "https://api.polygon.io"

def fetch_aggs_all(symbol, start_date, end_date, api_key, timespan="minute", multiplier=1):
    """
    Fetches all aggregate bars for a symbol within a date range from Polygon.io.
    Handles API pagination automatically.
    """
    print(f"Fetching '{symbol}' data from {start_date} to {end_date}...")
    url = (
        f"{POLYGON_API_BASE}/v2/aggs/ticker/{symbol}/range/{multiplier}/{timespan}/"
        f"{start_date}/{end_date}?adjusted=true&sort=asc&limit=50000"
    )
    
    headers = {"Authorization": f"Bearer {api_key}"}
    all_bars = []
    
    while url:
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            data = response.json()

            if "results" in data:
                all_bars.extend(data["results"])
                print(f" -> Fetched {len(data['results'])} bars...", end="\r")

            url = data.get("next_url")

        except requests.exceptions.RequestException as e:
            print(f"\nAPI Error fetching data for {symbol}: {e}")
            return None
        except Exception as e:
            print(f"\nAn unexpected error occurred: {e}")
            return None
            
    print(f"\n -> Total bars fetched for {symbol}: {len(all_bars)}")
    if not all_bars:
        return None
        
    # Convert to DataFrame for easier processing
    df = pd.DataFrame(all_bars)
    df.rename(columns={
        't': 'timestamp_utc_ms',
        'o': 'open',
        'h': 'high',
        'l': 'low',
        'c': 'close',
        'v': 'volume'
    }, inplace=True)
    return df

def filter_and_prepare_data(df):
    """
    Filters a DataFrame of market data for RTH (Regular Trading Hours)
    and removes US market holidays.
    """
    if df is None or df.empty:
        return None

    print("Filtering data for RTH and US market holidays...")
    
    # 1. Convert UTC millisecond timestamp to a timezone-aware DatetimeIndex
    df['timestamp_utc_ms'] = pd.to_datetime(df['timestamp_utc_ms'], unit='ms', utc=True)
    df.set_index('timestamp_utc_ms', inplace=True)
    
    # 2. Convert the index to New York time to perform RTH and holiday checks
    df.index = df.index.tz_convert(NY_TIMEZONE)
    
    # 3. Filter for Regular Trading Hours
    df = df.between_time(RTH_START, RTH_END)

    # 4. Filter out US market holidays
    nyse = mcal.get_calendar('NYSE')
    holidays = nyse.holidays().holidays # Get a list of holiday dates
    df = df[~df.index.normalize().isin(holidays)]
    
    print(f" -> {len(df)} bars remaining after filtering.")
    
    # 5. Add the specific columns required by the C++ backtester
    df['ts_utc'] = df.index.strftime('%Y-%m-%dT%H:%M:%S%z').str.replace(r'([+-])(\d{2})(\d{2})', r'\1\2:\3', regex=True)
    df['ts_nyt_epoch'] = df.index.astype('int64') // 10**9
    
    return df

def save_to_bin(df, path):
    """
    Saves the DataFrame to a custom binary format compatible with the C++ backtester.
    Format:
    - uint64_t: Number of bars
    - For each bar:
      - uint32_t: Length of ts_utc string
      - char[]: ts_utc string data
      - int64_t: ts_nyt_epoch
      - double: open, high, low, close
      - uint64_t: volume
    """
    print(f"Saving to binary format at {path}...")
    try:
        with open(path, 'wb') as f:
            # Write total number of bars
            num_bars = len(df)
            f.write(struct.pack('<Q', num_bars))

            # **FIXED**: The struct format string now correctly includes six format
            # specifiers to match the six arguments passed to pack().
            # q: int64_t (ts_nyt_epoch)
            # d: double (open)
            # d: double (high)
            # d: double (low)
            # d: double (close)
            # Q: uint64_t (volume)
            bar_struct = struct.Struct('<qddddQ')

            for row in df.itertuples():
                # Handle the variable-length string part
                ts_utc_bytes = row.ts_utc.encode('utf-8')
                f.write(struct.pack('<I', len(ts_utc_bytes)))
                f.write(ts_utc_bytes)
                
                # Pack and write the fixed-size data
                packed_data = bar_struct.pack(
                    row.ts_nyt_epoch,
                    row.open,
                    row.high,
                    row.low,
                    row.close,
                    int(row.volume) # C++ expects uint64_t, so we cast to int
                )
                f.write(packed_data)
        print(" -> Binary file saved successfully.")
    except Exception as e:
        print(f"Error saving binary file: {e}")

def main():
    parser = argparse.ArgumentParser(description="Polygon.io Data Downloader and Processor")
    parser.add_argument('symbols', nargs='+', help="One or more stock symbols (e.g., QQQ TQQQ SQQQ)")
    parser.add_argument('--start', required=True, help="Start date in YYYY-MM-DD format")
    parser.add_argument('--end', required=True, help="End date in YYYY-MM-DD format")
    parser.add_argument('--outdir', default='data', help="Output directory for CSV and BIN files")
    parser.add_argument('--timespan', default='minute', choices=['minute', 'hour', 'day'], help="Timespan of bars")
    parser.add_argument('--multiplier', default=1, type=int, help="Multiplier for the timespan")
    
    args = parser.parse_args()
    
    # Get API key from environment variable for security
    api_key = os.getenv('POLYGON_API_KEY')
    if not api_key:
        print("Error: POLYGON_API_KEY environment variable not set.")
        return
        
    # Create output directory if it doesn't exist
    output_dir = Path(args.outdir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for symbol in args.symbols:
        print("-" * 50)
        # 1. Fetch data
        df_raw = fetch_aggs_all(symbol, args.start, args.end, api_key, args.timespan, args.multiplier)
        
        if df_raw is None or df_raw.empty:
            print(f"No data fetched for {symbol}. Skipping.")
            continue
            
        # 2. Filter and prepare data
        df_clean = filter_and_prepare_data(df_raw)
        
        if df_clean is None or df_clean.empty:
            print(f"No data remaining for {symbol} after filtering. Skipping.")
            continue
        
        # 3. Define output paths
        file_prefix = f"{symbol.upper()}_RTH_NH"
        csv_path = output_dir / f"{file_prefix}.csv"
        bin_path = output_dir / f"{file_prefix}.bin"
        
        # 4. Save to CSV for inspection
        print(f"Saving to CSV format at {csv_path}...")
        # Select and order columns to match C++ struct for clarity
        csv_columns = ['ts_utc', 'ts_nyt_epoch', 'open', 'high', 'low', 'close', 'volume']
        df_clean[csv_columns].to_csv(csv_path, index=False)
        print(" -> CSV file saved successfully.")
        
        # 5. Save to C++ compatible binary format
        save_to_bin(df_clean, bin_path)

    print("-" * 50)
    print("Data download and processing complete.")

if __name__ == "__main__":
    main()


```

## üìÑ **FILE 12 of 26**: tools/dupdef_scan_cpp.py

**File Information**:
- **Path**: `tools/dupdef_scan_cpp.py`

- **Size**: 584 lines
- **Modified**: 2025-09-10 13:22:04

- **Type**: .py

```text
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
dupdef_scan_cpp.py ‚Äî detect duplicate C++ definitions (classes/functions/methods).

Features
--------
- Walks source tree; scans C/C++ headers/impl files.
- Strips comments and string/char literals safely.
- Finds:
  1) Duplicate class/struct/enum/union *definitions* (same fully-qualified name).
  2) Duplicate free functions and member functions *definitions* (same FQN + normalized signature).
  3) Flags identical-duplicate bodies vs. conflicting bodies (ODR risk).
- JSON or text output; CI-friendly nonzero exit with --fail-on-issues.

Heuristics
----------
- Lightweight parser (no libclang needed).
- Namespaces & nested classes tracked via a simple brace/namespace stack.
- Function signature normalization removes parameter names & defaults.
- Recognizes cv-qualifiers (const), ref-qualifiers (&, &&), noexcept, trailing return types.
- Ignores *declarations* (ends with ';'); only flags *definitions* (has '{...}').

Limitations
-----------
- It's a robust heuristic, not a full C++ parser. Works well for most codebases.
- Overloads: different normalized parameter types are *not* duplicates (OK).
- Inline/template functions: allowed across headers if body **identical** (configurable).

Usage
-----
  python dupdef_scan_cpp.py [paths...] \
      --exclude third_party --exclude build \
      --json-out dup_report.json --fail-on-issues

"""

from __future__ import annotations
import argparse, json, os, re, sys, hashlib, bisect
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Iterable

CPP_EXTS = {".h", ".hh", ".hpp", ".hxx", ".ipp",
            ".c", ".cc", ".cpp", ".cxx", ".cu", ".cuh"}

# ------------------ Utilities ------------------

def iter_files(paths: List[Path], exts=CPP_EXTS, excludes: List[str]=[]) -> Iterable[Path]:
    globs = [re.compile(fnmatch_to_re(pat)) for pat in excludes]
    for root in paths:
        if root.is_file():
            if root.suffix.lower() in exts and not any(g.search(str(root)) for g in globs):
                yield root
            continue
        for dirpath, dirnames, filenames in os.walk(root):
            full_dir = Path(dirpath)
            # skip excluded directories quickly
            if any(g.search(str(full_dir)) for g in globs):
                dirnames[:] = []  # don't descend
                continue
            for fn in filenames:
                p = full_dir / fn
                if p.suffix.lower() in exts and not any(g.search(str(p)) for g in globs):
                    yield p

def fnmatch_to_re(pat: str) -> str:
    # crude glob‚Üíregex (supports '*' and '**')
    pat = pat.replace(".", r"\.").replace("+", r"\+")
    pat = pat.replace("**/", r".*(/|^)").replace("**", r".*")
    pat = pat.replace("*", r"[^/]*").replace("?", r".")
    return r"^" + pat + r"$"

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", "ignore")).hexdigest()

# ------------------ C++ preprocessor: remove comments / strings ------------------

def strip_comments_and_strings(src: str) -> str:
    """
    Remove //... and /*...*/ and string/char literals while preserving newlines/positions.
    """
    out = []
    i, n = 0, len(src)
    NORMAL, SLASH, LINE, BLOCK, STR, CHAR = range(6)
    state = NORMAL
    quote = ""
    while i < n:
        c = src[i]
        if state == NORMAL:
            if c == '/':
                state = SLASH
                i += 1
                continue
            elif c == '"':
                state = STR; quote = '"'; out.append('"'); i += 1; continue
            elif c == "'":
                state = CHAR; quote = "'"; out.append("'"); i += 1; continue
            else:
                out.append(c); i += 1; continue

        elif state == SLASH:
            if i < n and src[i] == '/':
                state = LINE; out.append(' '); i += 1; continue
            elif i < n and src[i] == '*':
                state = BLOCK; out.append(' '); i += 1; continue
            else:
                # **Fix:** not a comment ‚Äî emit the prior '/' and reprocess current char in NORMAL.
                out.append('/')
                state = NORMAL
                continue

        elif state == LINE:
            if c == '\n':
                out.append('\n'); state = NORMAL
            else:
                out.append(' ')
            i += 1; continue

        elif state == BLOCK:
            if c == '*' and i+1 < n and src[i+1] == '/':
                out.append('  '); i += 2; state = NORMAL; continue
            out.append(' ' if c != '\n' else '\n'); i += 1; continue

        elif state in (STR, CHAR):
            if c == '\\':
                out.append('\\'); i += 1
                if i < n: out.append(' '); i += 1; continue
            out.append(quote if c == quote else ' ')
            if c == quote: state = NORMAL
            i += 1; continue

    return ''.join(out)

# ------------------ Lightweight C++ scanner ------------------

_id = r"[A-Za-z_]\w*"
ws = r"[ \t\r\n]*"

@dataclass
class ClassDef:
    fqname: str
    file: str
    line: int

@dataclass
class FuncDef:
    fqname: str
    params_norm: str  # normalized param types + cv/ref/noexcept
    file: str
    line: int
    body_hash: str
    is_inline_or_tpl: bool = False

@dataclass
class Findings:
    class_defs: Dict[str, List[ClassDef]] = field(default_factory=dict)
    func_defs: Dict[Tuple[str, str], List[FuncDef]] = field(default_factory=dict)  # (fqname, sig)->defs

    def add_class(self, c: ClassDef):
        self.class_defs.setdefault(c.fqname, []).append(c)

    def add_func(self, f: FuncDef):
        key = (f.fqname, f.params_norm)
        self.func_defs.setdefault(key, []).append(f)

def scan_cpp(text: str, fname: str) -> Findings:
    """
    Scan C++ source without full parse:
    - Tracks namespace stack.
    - Finds class/struct/enum/union names followed by '{' (definition).
    - Finds function/method definitions by header (...) { ... } and normalizes args.
    """
    stripped = strip_comments_and_strings(text)
    find = Findings()
    n = len(stripped)
    i = 0

    # Fast line number lookup
    nl_pos = [i for i, ch in enumerate(stripped) if ch == '\n']
    def line_of(pos: int) -> int:
        return bisect.bisect_right(nl_pos, pos) + 1

    ns_stack: List[str] = []
    class_stack: List[str] = []

    def skip_ws(k):
        while k < n and stripped[k] in " \t\r\n":
            k += 1
        return k

    def match_kw(k, kw):
        k = skip_ws(k)
        if stripped.startswith(kw, k) and (k+len(kw)==n or not stripped[k+len(kw)].isalnum() and stripped[k+len(kw)]!='_'):
            return k+len(kw)
        return -1

    def peek_ident_left(k):
        """backtrack from k (exclusive) to extract an identifier or X::Y qualified name"""
        j = k-1
        # skip spaces
        while j >= 0 and stripped[j].isspace(): j -= 1
        # now parse tokens backwards to assemble something like A::B::C
        tokens = []
        cur = []
        while j >= 0:
            ch = stripped[j]
            if ch.isalnum() or ch=='_' or ch in ['~', '>']:
                cur.append(ch); j -= 1; continue
            if ch == ':':
                # expect '::'
                if j-1 >= 0 and stripped[j-1]==':':
                    # finish current ident
                    ident = ''.join(reversed(cur)).strip()
                    if ident:
                        tokens.append(ident)
                    tokens.append('::')
                    cur = []
                    j -= 2
                    continue
                else:
                    break
            elif ch in " \t\r\n*&<>,":
                # end of ident piece
                if cur:
                    ident = ''.join(reversed(cur)).strip()
                    if ident:
                        tokens.append(ident)
                        cur=[]
                j -= 1
                # keep skipping qualifiers
                continue
            else:
                break
        if cur:
            tokens.append(''.join(reversed(cur)).strip())
        # tokens like ['Namespace', '::', 'Class', '::', 'func']
        tokens = list(reversed(tokens))
        # Clean consecutive '::'
        out = []
        for t in tokens:
            if t == '' or t == ',':
                continue
            out.append(t)
        name = ''.join(out).strip()
        return name

    def parse_balanced(k, open_ch='(', close_ch=')'):
        """ return (end_index_after_closer, content_inside) or (-1, '') """
        if k >= n or stripped[k] != open_ch:
            return -1, ''
        depth = 0
        j = k
        buf = []
        while j < n:
            ch = stripped[j]
            if ch == open_ch:
                depth += 1
            elif ch == close_ch:
                depth -= 1
                if depth == 0:
                    return j+1, ''.join(buf)
            buf.append(ch)
            j += 1
        return -1, ''

    def normalize_params(params: str, tail: str) -> str:
        # remove newline/extra spaces
        s = ' '.join(params.replace('\n',' ').replace('\r',' ').split())
        # drop default values
        s = re.sub(r"=\s*[^,)\[]+", "", s)
        # drop parameter names (heuristic: trailing identifier)
        parts = []
        depth = 0
        cur = []
        for ch in s:
            if ch == '<': depth += 1
            elif ch == '>': depth = max(0, depth-1)
            if ch == ',' and depth==0:
                parts.append(''.join(cur).strip())
                cur = []
            else:
                cur.append(ch)
        if cur: parts.append(''.join(cur).strip())
        norm_parts = []
        for p in parts:
            # remove trailing names (identifier possibly with [] or ref qualifiers)
            p = re.sub(r"\b([A-Za-z_]\w*)\s*(\[\s*\])*$", "", p).strip()
            p = re.sub(r"\s+", " ", p)
            # remove 'register'/'volatile' noise (keep const)
            p = re.sub(r"\b(register|volatile)\b", "", p).strip()
            norm_parts.append(p)
        args = ','.join(norm_parts)
        # tail qualifiers: const/noexcept/ref-qualifiers/-> trailing
        tail = tail.strip()
        # normalize spaces
        tail = ' '.join(tail.split())
        return args + ("|" + tail if tail else "")

    while i < n:
        # detect namespace blocks: namespace X { ... }
        j = skip_ws(i)
        if stripped.startswith("namespace", j):
            k = j + len("namespace")
            k = skip_ws(k)
            # anonymous namespace or named
            m = re.match(rf"{_id}", stripped[k:])
            if m:
                ns = m.group(0)
                k += len(ns)
            else:
                ns = ""  # anonymous
            k = skip_ws(k)
            if k < n and stripped[k] == '{':
                ns_stack.append(ns)
                i = k + 1
                continue

        # detect closing brace for namespace/class scopes to drop stacks
        if stripped[i] == '}':
            # pop class if needed (approximate: pop when we see '};' after class)
            # we don't strictly track braces per class; OK for duplication detection.
            if class_stack:
                class_stack.pop()
            if ns_stack:
                # only pop namespace if the previous open was a namespace (heuristic)
                # we can't easily distinguish; leave ns_stack pop conservative:
                ns_stack.pop()
            i += 1
            continue

        # class/struct/enum/union definitions
        for kw in ("class", "struct", "union", "enum class", "enum"):
            if stripped.startswith(kw, j) and re.match(r"\b", stripped[j+len(kw):]):
                k = j + len(kw)
                k = skip_ws(k)
                m = re.match(rf"{_id}", stripped[k:])
                if not m:
                    break
                name = m.group(0)
                k += len(name)
                # must be a definition if a '{' is ahead before ';'
                ahead = stripped[k:k+200]
                brace_pos = ahead.find('{')
                semi_pos  = ahead.find(';')
                if brace_pos != -1 and (semi_pos == -1 or brace_pos < semi_pos):
                    # capture FQN
                    fqn = '::'.join([n for n in ns_stack if n])  # ignore anonymous
                    if class_stack:
                        fqn = (fqn + ("::" if fqn else "") + "::".join(class_stack) + "::" + name) if fqn else "::".join(class_stack) + "::" + name
                    else:
                        fqn = (fqn + ("::" if fqn else "") + name) if fqn else name
                    line = line_of(j)
                    find.add_class(ClassDef(fqname=fqn, file=str(fname), line=line))
                    # push to class stack (best-effort)
                    class_stack.append(name)
                    i = j + 1
                    break
        # function/method definitions: look for (...) tail { ... }
        # Approach: find '(', parse to ')', then peek name before '(' and check body starts with '{'
        if stripped[i] == '(':
            # find header start: go back to name
            name = peek_ident_left(i)
            # skip false positives like if/for/switch/catch
            if name and not re.search(r"(?:^|::)(if|for|while|switch|catch|return)$", name):
                close_idx, inside = parse_balanced(i, '(', ')')
                if close_idx != -1:
                    # capture tail qualifiers + next token
                    k = skip_ws(close_idx)
                    tail_start = k
                    # consume possible 'const', 'noexcept', '&', '&&', trailing return
                    # don't consume '{' here
                    # trailing return '-> T'
                    # greedy but bounded
                    # collect until we hit '{' or ';'
                    while k < n and stripped[k] not in '{;':
                        k += 1
                    tail = stripped[tail_start:k]
                    # definition requires '{'
                    if k < n and stripped[k] == '{':
                        # Build FQN: include namespaces; for member methods prefixed with Class::method
                        # If name already qualified (contains '::'), use as-is with namespaces prefix only if name doesn't start with '::'
                        fqn = name
                        ns_prefix = '::'.join([n for n in ns_stack if n])
                        if '::' not in fqn.split('::')[0] and ns_prefix:
                            fqn = ns_prefix + "::" + fqn
                        params_norm = normalize_params(inside, tail)
                        # find body end brace
                        body_end = find_matching_brace(stripped, k)
                        body = stripped[k:body_end] if body_end != -1 else stripped[k:k+200]
                        body_hash = sha1(body)
                        # rough inline/template detection: preceding tokens include 'inline' or 'template<...>'
                        prefix = stripped[max(0, i-200):i]
                        is_inline = bool(re.search(r"\binline\b", prefix))
                        is_tpl = bool(re.search(r"\btemplate\s*<", prefix))
                        line = line_of(i)
                        find.add_func(FuncDef(fqname=fqn, params_norm=params_norm, file=str(fname),
                                              line=line, body_hash=body_hash,
                                              is_inline_or_tpl=(is_inline or is_tpl)))
                        i = k + 1
                        continue
            i += 1
            continue

        i += 1

    return find

def find_matching_brace(s: str, open_idx: int) -> int:
    """ given index of '{', return index after matching '}', ignoring braces in strings/comments (input already stripped). """
    if open_idx >= len(s) or s[open_idx] != '{': return -1
    depth = 0
    i = open_idx
    while i < len(s):
        ch = s[i]
        if ch == '{':
            depth += 1
        elif ch == '}':
            depth -= 1
            if depth == 0:
                return i + 1
        i += 1
    return -1

# ------------------ Report building ------------------

def merge_findings(allf: List[Findings]):
    classes: Dict[str, List[ClassDef]] = {}
    funcs: Dict[Tuple[str,str], List[FuncDef]] = {}
    for f in allf:
        for k, v in f.class_defs.items():
            classes.setdefault(k, []).extend(v)
        for k, v in f.func_defs.items():
            funcs.setdefault(k, []).extend(v)
    return classes, funcs

def build_report(classes, funcs, allow_identical_inline=True):
    duplicate_classes = []
    for fqname, defs in classes.items():
        # duplicate if defined in multiple *files*
        files = {d.file for d in defs}
        if len(files) > 1:
            duplicate_classes.append({
                "fqname": fqname,
                "defs": [{"file": d.file, "line": d.line} for d in defs]
            })

    duplicate_functions = []
    odr_conflicts = []
    for (fqname, sig), defs in funcs.items():
        if len(defs) <= 1: continue
        # group by body hash
        by_hash: Dict[str, List[FuncDef]] = {}
        for d in defs:
            by_hash.setdefault(d.body_hash, []).append(d)
        if len(by_hash) == 1:
            # identical bodies across files
            if allow_identical_inline:
                # only flag if defined in multiple DIFFERENT files and none are explicitly inline/template?
                if any(not d.is_inline_or_tpl for d in defs):
                    duplicate_functions.append({
                        "fqname": fqname, "signature": sig,
                        "kind": "identical_noninline",
                        "defs": [{"file": d.file, "line": d.line} for d in defs]
                    })
            else:
                duplicate_functions.append({
                    "fqname": fqname, "signature": sig,
                    "kind": "identical",
                    "defs": [{"file": d.file, "line": d.line} for d in defs]
                })
        else:
            # conflicting bodies ‚Äî ODR violation
            odr_conflicts.append({
                "fqname": fqname, "signature": sig,
                "variants": [
                    {"body_hash": h, "defs": [{"file": d.file, "line": d.line} for d in lst]}
                    for h, lst in by_hash.items()
                ]
            })

    return {
        "duplicate_classes": duplicate_classes,
        "duplicate_functions": duplicate_functions,
        "odr_conflicts": odr_conflicts,
    }

def print_report_text(report):
    out = []
    if report["duplicate_classes"]:
        out.append("== Duplicate class/struct/enum definitions ==")
        for item in report["duplicate_classes"]:
            out.append(f"  {item['fqname']}")
            for d in item["defs"]:
                out.append(f"    - {d['file']}:{d['line']}")
    if report["duplicate_functions"]:
        out.append("== Duplicate function/method definitions (identical bodies) ==")
        for item in report["duplicate_functions"]:
            out.append(f"  {item['fqname']}({item['signature']}) [{item.get('kind','identical')}]")
            for d in item["defs"]:
                out.append(f"    - {d['file']}:{d['line']}")
    if report["odr_conflicts"]:
        out.append("== Conflicting function/method definitions (ODR risk) ==")
        for item in report["odr_conflicts"]:
            out.append(f"  {item['fqname']}({item['signature']})")
            for var in item["variants"]:
                out.append(f"    body {var['body_hash'][:12]}:")
                for d in var["defs"]:
                    out.append(f"      - {d['file']}:{d['line']}")
    if not out:
        out.append("No duplicate C++ definitions found.")
    return "\n".join(out) + "\n"

# ------------------ CLI ------------------

def parse_args(argv=None):
    ap = argparse.ArgumentParser(description="Scan C++ codebase for duplicate definitions.")
    ap.add_argument("paths", nargs="*", default=["."], help="Files or directories to scan.")
    ap.add_argument("--exclude", action="append", default=[],
                    help="Glob/regex to exclude (e.g. 'build/**', 'third_party/**').")
    ap.add_argument("--json-out", default=None, help="Write JSON report to file.")
    ap.add_argument("--allow-identical-inline", action="store_true", default=True,
                    help="Allow identical inline/template function bodies across headers (default).")
    ap.add_argument("--no-allow-identical-inline", dest="allow_identical_inline",
                    action="store_false", help="Flag identical inline/template duplicates too.")
    ap.add_argument("--fail-on-issues", action="store_true", help="Exit 2 if any issues found.")
    ap.add_argument("--max-file-size-mb", type=int, default=5, help="Skip files bigger than this.")
    ap.add_argument("--jobs", type=int, default=0,
                    help="Number of parallel processes for scanning (0 = auto, 1 = no parallel).")
    return ap.parse_args(argv)

def scan_one_file(path: str, max_mb: int):
    p = Path(path)
    if p.stat().st_size > max_mb * 1024 * 1024:
        return None
    try:
        text = p.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        return ("warn", f"[WARN] Could not read {p}: {e}")
    f = scan_cpp(text, str(p))
    return ("ok", f)

def main(argv=None):
    args = parse_args(argv)
    roots = [Path(p).resolve() for p in args.paths]
    files = list(iter_files(roots, exts=CPP_EXTS, excludes=args.exclude))
    all_findings: List[Findings] = []

    jobs = (os.cpu_count() or 2) if args.jobs == 0 else max(1, args.jobs)
    if jobs <= 1:
        for f in files:
            res = scan_one_file(str(f), args.max_file_size_mb)
            if res is None: continue
            kind, payload = res
            if kind == "warn": print(payload, file=sys.stderr); continue
            all_findings.append(payload)
    else:
        with ProcessPoolExecutor(max_workers=jobs) as ex:
            futs = {ex.submit(scan_one_file, str(f), args.max_file_size_mb): f for f in files}
            for fut in as_completed(futs):
                res = fut.result()
                if res is None: continue
                kind, payload = res
                if kind == "warn": print(payload, file=sys.stderr); continue
                all_findings.append(payload)

    classes, funcs = merge_findings(all_findings)
    report = build_report(classes, funcs, allow_identical_inline=args.allow_identical_inline)

    out_text = print_report_text(report)
    if args.json_out:
        with open(args.json_out, "w", encoding="utf-8") as fp:
            json.dump(report, fp, indent=2)
        sys.stdout.write(out_text)
    else:
        sys.stdout.write(out_text)

    if args.fail_on_issues:
        has_issues = bool(report["duplicate_classes"] or report["duplicate_functions"] or report["odr_conflicts"])
        raise SystemExit(2 if has_issues else 0)

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 13 of 26**: tools/emit_last10_trades.py

**File Information**:
- **Path**: `tools/emit_last10_trades.py`

- **Size**: 65 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
import sys, json
from datetime import datetime, timezone
from collections import defaultdict

def main():
    if len(sys.argv) < 2:
        print("Usage: python tools/emit_last10_trades.py <audit.jsonl>")
        sys.exit(1)
    path = sys.argv[1]
    fills = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                # Some audit lines append a trailing ,"sha1":"..." after the JSON object.
                # Trim that suffix if present to obtain a valid JSON object.
                raw = line
                cut = raw.find('},"sha1"')
                if cut != -1:
                    raw = raw[:cut+1]
                ev = json.loads(raw)
            except Exception:
                continue
            if ev.get("type") == "fill":
                ts = int(ev.get("ts", 0))
                d = datetime.fromtimestamp(ts, tz=timezone.utc)
                day = d.date().isoformat()
                fills.append((ts, day, ev))
    if not fills:
        print("No fills found.")
        return
    fills.sort(key=lambda x: x[0])
    # collect last 10 distinct days
    days_ordered = []
    seen = set()
    for _, day, _ in fills:
        if day not in seen:
            seen.add(day)
            days_ordered.append(day)
    last10 = set(days_ordered[-10:])
    # group by day
    by_day = defaultdict(list)
    for ts, day, ev in fills:
        if day in last10:
            by_day[day].append((ts, ev))
    for day in sorted(by_day.keys()):
        print(f"=== {day} ===")
        for ts, ev in sorted(by_day[day], key=lambda x: x[0]):
            t = datetime.fromtimestamp(ts, tz=timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
            inst = ev.get("inst", "")
            side = ev.get("side")
            side_s = "BUY" if side in (0, "Buy") else ("SELL" if side in (1, "Sell") else str(side))
            qty = float(ev.get("qty", 0.0))
            px = float(ev.get("px", 0.0))
            fees = float(ev.get("fees", 0.0))
            pnl_d = float(ev.get("pnl_d", 0.0))
            pos_after = float(ev.get("pos_after", 0.0))
            eq_after = float(ev.get("eq_after", 0.0))
            print(f"{t} {inst:5s} {side_s:4s} qty={qty:.4f} px={px:.4f} fees={fees:.4f} pnl_d={pnl_d:.4f} pos_after={pos_after:.4f} eq_after={eq_after:.2f}")

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 14 of 26**: tools/extract_instrument_distribution.py

**File Information**:
- **Path**: `tools/extract_instrument_distribution.py`

- **Size**: 59 lines
- **Modified**: 2025-09-10 14:41:22

- **Type**: .py

```text
#!/usr/bin/env python3
"""
Quick script to extract instrument distribution from audit files.
"""

import sys
import json
from collections import Counter

def extract_instrument_distribution(audit_file):
    """Extract instrument distribution from audit file."""
    
    instruments = []
    
    with open(audit_file, 'r') as f:
        for line in f:
            try:
                # Extract JSON part before sha1
                json_part = line.strip()
                if '","sha1":"' in json_part:
                    json_part = json_part.split('","sha1":"')[0] + '"}'
                elif ',"sha1":"' in json_part:
                    json_part = json_part.split(',"sha1":"')[0] + '}'
                
                event = json.loads(json_part)
                
                # Only count fill events
                if event.get('type') == 'fill':
                    instrument = event.get('inst', 'UNKNOWN')
                    instruments.append(instrument)
                    
            except (json.JSONDecodeError, KeyError):
                continue
    
    # Count and display
    counter = Counter(instruments)
    total = sum(counter.values())
    
    print(f"üìä Instrument Distribution Analysis")
    print(f"üìÅ File: {audit_file}")
    print(f"üìà Total Trades: {total}")
    print()
    
    for instrument, count in counter.most_common():
        percentage = (count / total * 100) if total > 0 else 0
        print(f"{instrument:>6}: {count:>4} trades ({percentage:>5.1f}%)")
    
    return counter

def main():
    if len(sys.argv) != 2:
        print("Usage: python extract_instrument_distribution.py <audit_file>")
        sys.exit(1)
    
    audit_file = sys.argv[1]
    extract_instrument_distribution(audit_file)

if __name__ == '__main__':
    main()

```

## üìÑ **FILE 15 of 26**: tools/fast_historical_bridge.py

**File Information**:
- **Path**: `tools/fast_historical_bridge.py`

- **Size**: 196 lines
- **Modified**: 2025-09-11 13:34:13

- **Type**: .py

```text
#!/usr/bin/env python3
"""
Fast Historical Bridge - Optimized for speed without MarS complexity

This generates realistic market data using historical patterns but without
the overhead of MarS simulation engine.
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import argparse
import pytz

def load_historical_data(filepath: str, recent_days: int = 30) -> pd.DataFrame:
    """Load and process historical data efficiently."""
    df = pd.read_csv(filepath)
    
    # Handle different timestamp formats
    timestamp_col = 'timestamp' if 'timestamp' in df.columns else 'ts_utc'
    df['timestamp'] = pd.to_datetime(df[timestamp_col], utc=True)
    
    # Use only recent data for faster processing
    if len(df) > recent_days * 390:  # ~390 bars per day
        df = df.tail(recent_days * 390)
        # Note: Debug print removed for quiet mode compatibility
    
    return df

def analyze_historical_patterns(df: pd.DataFrame) -> Dict[str, Any]:
    """Analyze historical patterns for realistic generation."""
    patterns = {}
    
    # Price statistics
    returns = np.diff(np.log(df['close']))
    patterns['mean_return'] = np.mean(returns)
    patterns['volatility'] = np.std(returns)
    patterns['price_trend'] = (df['close'].iloc[-1] - df['close'].iloc[0]) / len(df)
    
    # Volume statistics
    patterns['mean_volume'] = df['volume'].mean()
    patterns['volume_std'] = df['volume'].std()
    patterns['volume_trend'] = (df['volume'].iloc[-1] - df['volume'].iloc[0]) / len(df)
    
    # Intraday patterns
    df['hour'] = df['timestamp'].dt.hour
    hourly_stats = df.groupby('hour').agg({
        'volume': 'mean',
        'close': lambda x: np.std(np.diff(np.log(x)))
    }).reset_index()
    
    patterns['hourly_volume_multipliers'] = {}
    patterns['hourly_volatility_multipliers'] = {}
    
    for _, row in hourly_stats.iterrows():
        hour = int(row['hour'])
        patterns['hourly_volume_multipliers'][hour] = row['volume'] / patterns['mean_volume']
        patterns['hourly_volatility_multipliers'][hour] = row['close'] / patterns['volatility']
    
    # Fill missing hours
    for hour in range(24):
        if hour not in patterns['hourly_volume_multipliers']:
            patterns['hourly_volume_multipliers'][hour] = 1.0
            patterns['hourly_volatility_multipliers'][hour] = 1.0
    
    return patterns

def generate_realistic_bars(
    patterns: Dict[str, Any],
    start_price: float,
    duration_minutes: int,
    bar_interval_seconds: int = 60,
    symbol: str = "QQQ"
) -> List[Dict[str, Any]]:
    """Generate realistic market bars using historical patterns."""
    
    bars = []
    current_price = start_price
    
    # Always start from today's market open time (9:30 AM ET)
    et_tz = pytz.timezone('US/Eastern')
    today_et = datetime.now(et_tz)
    
    # Market open time (9:30 AM ET) - always use today's open
    current_time = today_et.replace(hour=9, minute=30, second=0, microsecond=0)
    # Note: Debug print removed for quiet mode compatibility
    
    # Convert to UTC for consistent timestamp generation
    current_time = current_time.astimezone(pytz.UTC)
    
    num_bars = duration_minutes * 60 // bar_interval_seconds
    
    for i in range(num_bars):
        # Apply time-of-day patterns
        hour = current_time.hour
        volume_multiplier = patterns['hourly_volume_multipliers'].get(hour, 1.0)
        volatility_multiplier = patterns['hourly_volatility_multipliers'].get(hour, 1.0)
        
        # Generate realistic price movement
        price_change = np.random.normal(
            patterns['mean_return'], 
            patterns['volatility'] * volatility_multiplier
        )
        current_price *= (1 + price_change)
        
        # Generate OHLC
        volatility = patterns['volatility'] * volatility_multiplier * current_price
        high_price = current_price + np.random.exponential(volatility * 0.5)
        low_price = current_price - np.random.exponential(volatility * 0.5)
        open_price = current_price + np.random.normal(0, volatility * 0.1)
        close_price = current_price
        
        # Generate realistic volume
        base_volume = patterns['mean_volume'] * volume_multiplier
        volume = int(np.random.lognormal(np.log(base_volume), 0.3))
        volume = max(1000, min(volume, 1000000))  # Reasonable bounds
        
        bar = {
            "timestamp": int(current_time.timestamp()),
            "open": round(open_price, 2),
            "high": round(high_price, 2),
            "low": round(low_price, 2),
            "close": round(close_price, 2),
            "volume": volume,
            "symbol": symbol
        }
        
        bars.append(bar)
        current_time += timedelta(seconds=bar_interval_seconds)
    
    return bars

def main():
    parser = argparse.ArgumentParser(description="Fast Historical Bridge for Market Data Generation")
    parser.add_argument("--symbol", default="QQQ", help="Symbol to simulate")
    parser.add_argument("--historical-data", required=True, help="Path to historical CSV data file")
    parser.add_argument("--continuation-minutes", type=int, default=60, help="Minutes to generate")
    parser.add_argument("--recent-days", type=int, default=30, help="Days of recent data to use")
    parser.add_argument("--output", default="fast_historical_data.json", help="Output filename")
    parser.add_argument("--format", default="json", choices=["json", "csv"], help="Output format")
    parser.add_argument("--quiet", action="store_true", help="Suppress debug output")
    
    args = parser.parse_args()
    
    if not args.quiet:
        print(f"üöÄ Fast Historical Bridge - {args.symbol}")
        print(f"üìä Historical data: {args.historical_data}")
        print(f"‚è±Ô∏è  Duration: {args.continuation_minutes} minutes")
    
    # Load and analyze historical data
    if not args.quiet:
        print("üìà Loading historical data...")
    df = load_historical_data(args.historical_data, args.recent_days)
    
    if not args.quiet:
        print("üîç Analyzing historical patterns...")
    patterns = analyze_historical_patterns(df)
    
    # Generate realistic data
    if not args.quiet:
        print("üé≤ Generating realistic market data...")
    start_price = df['close'].iloc[-1]
    bars = generate_realistic_bars(
        patterns=patterns,
        start_price=start_price,
        duration_minutes=args.continuation_minutes,
        symbol=args.symbol
    )
    
    # Export data
    if args.format == "csv":
        df_output = pd.DataFrame(bars)
        df_output.to_csv(args.output, index=False)
    else:
        with open(args.output, 'w') as f:
            json.dump(bars, f, indent=2)
    
    if not args.quiet:
        print(f"‚úÖ Generated {len(bars)} bars")
        print(f"üìà Price range: ${min(bar['low'] for bar in bars):.2f} - ${max(bar['high'] for bar in bars):.2f}")
        print(f"üìä Volume range: {min(bar['volume'] for bar in bars):,} - {max(bar['volume'] for bar in bars):,}")
        
        # Show time range in Eastern Time for clarity
        if bars:
            start_time = datetime.fromtimestamp(bars[0]['timestamp'], tz=pytz.UTC)
            end_time = datetime.fromtimestamp(bars[-1]['timestamp'], tz=pytz.UTC)
            start_time_et = start_time.astimezone(pytz.timezone('US/Eastern'))
            end_time_et = end_time.astimezone(pytz.timezone('US/Eastern'))
            print(f"‚è∞ Time range: {start_time_et.strftime('%Y-%m-%d %H:%M:%S %Z')} to {end_time_et.strftime('%Y-%m-%d %H:%M:%S %Z')}")
        
        print(f"üíæ Saved to: {args.output}")

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 16 of 26**: tools/finalize_kochi_features.py

**File Information**:
- **Path**: `tools/finalize_kochi_features.py`

- **Size**: 52 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
import argparse
import csv
import hashlib
import json
import pathlib


def sha256_bytes(b: bytes) -> str:
    return "sha256:" + hashlib.sha256(b).hexdigest()


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--csv", required=True, help="Kochi features CSV (timestamp + feature columns)")
    ap.add_argument("--seq_len", type=int, default=64)
    ap.add_argument("--emit_from", type=int, default=64)
    ap.add_argument("--pad_value", type=float, default=0.0)
    ap.add_argument("--out", default="configs/features/kochi_v1_spec.json")
    args = ap.parse_args()

    p = pathlib.Path(args.csv)
    with open(p, newline="") as fh:
        reader = csv.reader(fh)
        header = next(reader)

    names = [c for c in header if c.lower() not in ("ts", "timestamp", "bar_index", "time")]

    spec = {
        "family": "KOCHI",
        "version": "v1",
        "input_dim": len(names),
        "seq_len": int(args.seq_len),
        "emit_from": int(args.emit_from),
        "pad_value": float(args.pad_value),
        "names": names,
        "provenance": {
            "source_csv": str(p),
            "header_hash": sha256_bytes(",".join(header).encode()),
        },
        "ops": {"note": "Kochi features supplied externally; no op list"},
    }

    outp = pathlib.Path(args.out)
    outp.parent.mkdir(parents=True, exist_ok=True)
    outp.write_text(json.dumps(spec, indent=2))
    print(f"‚úÖ Wrote Kochi feature spec ‚Üí {outp} (F={len(names)}, T={args.seq_len})")


if __name__ == "__main__":
    main()



```

## üìÑ **FILE 17 of 26**: tools/generate_bar_sequence.cpp

**File Information**:
- **Path**: `tools/generate_bar_sequence.cpp`

- **Size**: 106 lines
- **Modified**: 2025-09-11 02:42:31

- **Type**: .cpp

```text
#include "sentio/virtual_market.hpp"
#include <iostream>
#include <iomanip>
#include <fstream>

int main() {
    // Create virtual market engine
    sentio::VirtualMarketEngine vm_engine;
    
    // Generate 100 bars for QQQ in normal market conditions
    std::vector<sentio::Bar> bars = vm_engine.generate_market_data("QQQ", 100, 60);
    
    // Output bar stream to console
    std::cout << "QQQ Virtual Market Bar Sequence (100 bars, Normal Market)" << std::endl;
    std::cout << "=================================================================" << std::endl;
    std::cout << std::setw(8) << "Bar" 
              << std::setw(12) << "Timestamp" 
              << std::setw(10) << "Open" 
              << std::setw(10) << "High" 
              << std::setw(10) << "Low" 
              << std::setw(10) << "Close" 
              << std::setw(10) << "Volume" 
              << std::setw(12) << "Return%" << std::endl;
    std::cout << "=================================================================" << std::endl;
    
    double prev_close = 0.0;
    for (size_t i = 0; i < bars.size(); ++i) {
        const auto& bar = bars[i];
        
        // Calculate return percentage
        double return_pct = 0.0;
        if (i > 0 && prev_close > 0) {
            return_pct = ((bar.close - prev_close) / prev_close) * 100.0;
        }
        
        // Convert timestamp to readable format (simplified)
        std::time_t timestamp = static_cast<std::time_t>(bar.ts_utc_epoch);
        std::tm* tm_info = std::localtime(&timestamp);
        
        std::cout << std::setw(8) << (i + 1)
                  << std::setw(12) << bar.ts_utc_epoch
                  << std::setw(10) << std::fixed << std::setprecision(2) << bar.open
                  << std::setw(10) << std::fixed << std::setprecision(2) << bar.high
                  << std::setw(10) << std::fixed << std::setprecision(2) << bar.low
                  << std::setw(10) << std::fixed << std::setprecision(2) << bar.close
                  << std::setw(10) << std::fixed << std::setprecision(0) << bar.volume
                  << std::setw(12) << std::fixed << std::setprecision(3) << return_pct << "%" << std::endl;
        
        prev_close = bar.close;
    }
    
    // Also save to CSV file for easy analysis
    std::ofstream csv_file("qqq_virtual_bars.csv");
    csv_file << "bar,timestamp,open,high,low,close,volume,return_pct\n";
    
    prev_close = 0.0;
    for (size_t i = 0; i < bars.size(); ++i) {
        const auto& bar = bars[i];
        
        double return_pct = 0.0;
        if (i > 0 && prev_close > 0) {
            return_pct = ((bar.close - prev_close) / prev_close) * 100.0;
        }
        
        csv_file << (i + 1) << ","
                 << bar.ts_utc_epoch << ","
                 << std::fixed << std::setprecision(2) << bar.open << ","
                 << std::fixed << std::setprecision(2) << bar.high << ","
                 << std::fixed << std::setprecision(2) << bar.low << ","
                 << std::fixed << std::setprecision(2) << bar.close << ","
                 << std::fixed << std::setprecision(0) << bar.volume << ","
                 << std::fixed << std::setprecision(3) << return_pct << "\n";
        
        prev_close = bar.close;
    }
    csv_file.close();
    
    std::cout << "\n=================================================================" << std::endl;
    std::cout << "Bar sequence saved to: qqq_virtual_bars.csv" << std::endl;
    std::cout << "Total bars generated: " << bars.size() << std::endl;
    
    // Calculate some basic statistics
    if (!bars.empty()) {
        double total_return = ((bars.back().close - bars.front().open) / bars.front().open) * 100.0;
        std::cout << "Total return over sequence: " << std::fixed << std::setprecision(3) << total_return << "%" << std::endl;
        
        // Calculate volatility
        double sum_returns = 0.0, sum_squared_returns = 0.0;
        int valid_returns = 0;
        for (size_t i = 1; i < bars.size(); ++i) {
            double ret = (bars[i].close - bars[i-1].close) / bars[i-1].close;
            sum_returns += ret;
            sum_squared_returns += ret * ret;
            valid_returns++;
        }
        
        if (valid_returns > 0) {
            double mean_return = sum_returns / valid_returns;
            double variance = (sum_squared_returns / valid_returns) - (mean_return * mean_return);
            double volatility = std::sqrt(variance) * 100.0;
            std::cout << "Average daily volatility: " << std::fixed << std::setprecision(3) << volatility << "%" << std::endl;
        }
    }
    
    return 0;
}

```

## üìÑ **FILE 18 of 26**: tools/generate_feature_cache.py

**File Information**:
- **Path**: `tools/generate_feature_cache.py`

- **Size**: 79 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
import argparse, json, hashlib, pathlib, numpy as np
import pandas as pd
import sentio_features as sf

def spec_with_hash(p):
    raw = pathlib.Path(p).read_bytes()
    spec = json.loads(raw)
    spec["content_hash"] = "sha256:" + hashlib.sha256(json.dumps(spec, sort_keys=True).encode()).hexdigest()
    return spec

def load_bars(csv_path):
    df = pd.read_csv(csv_path, low_memory=False)
    if "ts" in df.columns:
        ts = pd.to_datetime(df["ts"], utc=True, errors="coerce").astype("int64") // 10**9
    elif "ts_nyt_epoch" in df.columns:
        ts = df["ts_nyt_epoch"].astype("int64")
    elif "ts_utc_epoch" in df.columns:
        ts = df["ts_utc_epoch"].astype("int64")
    else:
        raise ValueError("No timestamp column found. Available columns: %s" % list(df.columns))
    mask = ts.notna()
    ts = ts[mask].astype(np.int64)
    df = df.loc[mask]
    return (
        ts.to_numpy(np.int64),
        df["open"].astype(float).to_numpy(),
        df["high"].astype(float).to_numpy(),
        df["low"].astype(float).to_numpy(),
        df["close"].astype(float).to_numpy(),
        df["volume"].astype(float).to_numpy(),
    )

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", required=True, help="Base ticker, e.g. QQQ")
    ap.add_argument("--bars", required=True, help="CSV with columns: ts,open,high,low,close,volume")
    ap.add_argument("--spec", required=True, help="feature_spec_55.json")
    ap.add_argument("--outdir", default="data", help="output dir for features files")
    args = ap.parse_args()

    outdir = pathlib.Path(args.outdir); outdir.mkdir(parents=True, exist_ok=True)
    spec = spec_with_hash(args.spec); spec_json = json.dumps(spec, sort_keys=True)
    ts, o, h, l, c, v = load_bars(args.bars)

    print(f"[FeatureCache] Building features for {args.symbol} with {len(ts)} bars...")
    X = sf.build_features_from_spec(args.symbol, ts, o, h, l, c, v, spec_json).astype(np.float32)
    N, F = X.shape
    names = [f.get("name", f'{f["op"]}_{f.get("source","")}_{f.get("window","")}_{f.get("k","")}') for f in spec["features"]]

    print(f"[FeatureCache] Generated features: {N} rows x {F} features")
    print(f"[FeatureCache] Feature stats: min={X.min():.6f}, max={X.max():.6f}, mean={X.mean():.6f}, std={X.std():.6f}")

    csv_path = outdir / f"{args.symbol}_RTH_features.csv"
    header = "bar_index,timestamp," + ",".join(names)
    M = np.empty((N, F+2), dtype=np.float32)
    M[:, 0] = np.arange(N).astype(np.float64)
    M[:, 1] = ts.astype(np.float64)
    M[:, 2:] = X
    np.savetxt(csv_path, M, delimiter=",", header=header, comments="", fmt="%.6f")
    print(f"‚úÖ CSV saved: {csv_path}")

    npy_path = outdir / f"{args.symbol}_RTH_features.npy"
    np.save(npy_path, X, allow_pickle=False)
    print(f"‚úÖ NPY saved: {npy_path}")

    meta = {
        "schema_version":"1.0",
        "symbol": args.symbol,
        "rows": int(N), "cols": int(F),
        "feature_names": names,
        "spec_hash": spec["content_hash"],
        "emit_from": int(spec["alignment_policy"]["emit_from_index"])
    }
    json.dump(meta, open(outdir / f"{args.symbol}_RTH_features.meta.json","w"), indent=2)
    print(f"‚úÖ META saved: {outdir / (args.symbol + '_RTH_features.meta.json')}")

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 19 of 26**: tools/generate_kochi_feature_cache.py

**File Information**:
- **Path**: `tools/generate_kochi_feature_cache.py`

- **Size**: 80 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
import argparse
import json
import pathlib
import numpy as np
import pandas as pd

import sentio_features as sf


def load_bars_csv(csv_path: str):
    df = pd.read_csv(csv_path, low_memory=False)
    if "ts" in df.columns:
        ts = pd.to_datetime(df["ts"], utc=True, errors="coerce").astype("int64") // 10**9
    elif "ts_nyt_epoch" in df.columns:
        ts = df["ts_nyt_epoch"].astype("int64")
    elif "ts_utc_epoch" in df.columns:
        ts = df["ts_utc_epoch"].astype("int64")
    else:
        raise ValueError("No timestamp column found in bars CSV")
    # Drop any bad rows
    mask = ts.notna()
    ts = ts[mask].astype(np.int64)
    df = df.loc[mask]
    o = df["open"].astype(float).to_numpy()
    h = df["high"].astype(float).to_numpy()
    l = df["low"].astype(float).to_numpy()
    c = df["close"].astype(float).to_numpy()
    v = df["volume"].astype(float).to_numpy()
    return ts.to_numpy(np.int64), o, h, l, c, v


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--symbol", required=True)
    ap.add_argument("--bars", required=True)
    ap.add_argument("--outdir", default="data")
    args = ap.parse_args()

    outdir = pathlib.Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    ts, o, h, l, c, v = load_bars_csv(args.bars)
    cols = sf.kochi_feature_names()
    X = sf.build_kochi_features(ts, o, h, l, c, v)

    # Save CSV: bar_index,timestamp,<features>
    csv_path = outdir / f"{args.symbol}_KOCHI_features.csv"
    header = "bar_index,timestamp," + ",".join(cols)
    N, F = int(X.shape[0]), int(X.shape[1])
    M = np.empty((N, F + 2), dtype=np.float64)
    M[:, 0] = np.arange(N, dtype=np.float64)
    M[:, 1] = ts.astype(np.float64)
    M[:, 2:] = X
    np.savetxt(csv_path, M, delimiter=",", header=header, comments="",
               fmt="%.10g")

    # Save NPY (feature matrix only)
    npy_path = outdir / f"{args.symbol}_KOCHI_features.npy"
    np.save(npy_path, X.astype(np.float32), allow_pickle=False)

    # Save META
    meta = {
        "schema_version": "1.0",
        "symbol": args.symbol,
        "rows": int(N),
        "cols": int(F),
        "feature_names": cols,
        "emit_from": 0,
        "kind": "kochi_features",
    }
    meta_path = outdir / f"{args.symbol}_KOCHI_features.meta.json"
    json.dump(meta, open(meta_path, "w"), indent=2)
    print(f"‚úÖ Wrote: {csv_path}\n‚úÖ Wrote: {npy_path}\n‚úÖ Wrote: {meta_path}")


if __name__ == "__main__":
    main()



```

## üìÑ **FILE 20 of 26**: tools/historical_context_agent.py

**File Information**:
- **Path**: `tools/historical_context_agent.py`

- **Size**: 392 lines
- **Modified**: 2025-09-11 03:29:16

- **Type**: .py

```text
#!/usr/bin/env python3
"""
HistoricalContextAgent for MarS Integration

This agent uses real historical market data to establish realistic market conditions,
then transitions to MarS's AI-powered generation for continuation.
"""

import random
import numpy as np
from typing import List, Dict, Any, Optional
from pandas import Timestamp, Timedelta
import pandas as pd

# Add MarS to Python path
import sys
from pathlib import Path
mars_path = Path(__file__).parent.parent / "MarS"
sys.path.insert(0, str(mars_path))

from mlib.core.action import Action
from mlib.core.base_agent import BaseAgent
from mlib.core.observation import Observation
from mlib.core.limit_order import LimitOrder

class HistoricalBar:
    """Represents a historical market bar."""
    def __init__(self, timestamp: Timestamp, open: float, high: float, 
                 low: float, close: float, volume: int):
        self.timestamp = timestamp
        self.open = open
        self.high = high
        self.low = low
        self.close = close
        self.volume = volume
        self.range = high - low
        self.body = abs(close - open)
        self.upper_shadow = high - max(open, close)
        self.lower_shadow = min(open, close) - low

class HistoricalContextAgent(BaseAgent):
    """
    Agent that uses historical market data to establish realistic market conditions,
    then transitions to synthetic generation.
    """
    
    def __init__(
        self,
        symbol: str,
        historical_bars: List[HistoricalBar],
        continuation_minutes: int = 60,
        start_time: Timestamp = None,
        end_time: Timestamp = None,
        seed: int = 42,
        transition_mode: str = "gradual"  # "gradual" or "immediate"
    ):
        super().__init__(
            init_cash=1000000,  # Large cash for market making
            communication_delay=0,
            computation_delay=0,
        )
        
        self.symbol = symbol
        self.historical_bars = historical_bars
        self.continuation_minutes = continuation_minutes
        self.transition_mode = transition_mode
        self.rnd = random.Random(seed)
        
        # Time management - ensure all timestamps are timezone-aware
        self.start_time = start_time or historical_bars[0].timestamp
        self.historical_end_time = historical_bars[-1].timestamp
        self.end_time = end_time or (self.historical_end_time + Timedelta(minutes=continuation_minutes))
        
        # Ensure all times are timezone-aware (UTC)
        if self.start_time.tz is None:
            self.start_time = self.start_time.tz_localize('UTC')
        if self.historical_end_time.tz is None:
            self.historical_end_time = self.historical_end_time.tz_localize('UTC')
        if self.end_time.tz is None:
            self.end_time = self.end_time.tz_localize('UTC')
        
        # State tracking
        self.current_bar_index = 0
        self.current_price = historical_bars[-1].close
        self.price_history = [bar.close for bar in historical_bars]
        self.volume_history = [bar.volume for bar in historical_bars]
        
        # Market statistics for realistic generation
        self._calculate_market_stats()
        
        # Transition state
        self.in_transition = False
        self.transition_start_time = None
        
    def _calculate_market_stats(self):
        """Calculate market statistics from historical data."""
        if len(self.price_history) < 2:
            return
            
        # Price statistics
        returns = np.diff(np.log(self.price_history))
        self.mean_return = np.mean(returns)
        self.volatility = np.std(returns)
        self.price_trend = (self.price_history[-1] - self.price_history[0]) / len(self.price_history)
        
        # Volume statistics
        self.mean_volume = np.mean(self.volume_history)
        self.volume_std = np.std(self.volume_history)
        self.volume_trend = (self.volume_history[-1] - self.volume_history[0]) / len(self.volume_history)
        
        # Intraday patterns
        self._analyze_intraday_patterns()
        
    def _analyze_intraday_patterns(self):
        """Analyze intraday volume and volatility patterns."""
        hourly_volumes = {}
        hourly_volatilities = {}
        
        for bar in self.historical_bars:
            hour = bar.timestamp.hour
            if hour not in hourly_volumes:
                hourly_volumes[hour] = []
                hourly_volatilities[hour] = []
            
            hourly_volumes[hour].append(bar.volume)
            if bar.range > 0:
                hourly_volatilities[hour].append(bar.range / bar.close)
        
        # Calculate hourly multipliers
        self.hourly_volume_multipliers = {}
        self.hourly_volatility_multipliers = {}
        
        for hour in range(24):
            if hour in hourly_volumes:
                self.hourly_volume_multipliers[hour] = np.mean(hourly_volumes[hour]) / self.mean_volume
                self.hourly_volatility_multipliers[hour] = np.mean(hourly_volatilities[hour]) / self.volatility
            else:
                self.hourly_volume_multipliers[hour] = 1.0
                self.hourly_volatility_multipliers[hour] = 1.0
    
    def get_action(self, observation: Observation) -> Action:
        """Generate action based on current time and mode."""
        time = observation.time
        
        if time < self.start_time:
            return Action(agent_id=self.agent_id, orders=[], time=time, 
                         next_wakeup_time=self.start_time)
        
        if time > self.end_time:
            return Action(agent_id=self.agent_id, orders=[], time=time, 
                         next_wakeup_time=None)
        
        # Determine if we're in historical replay or continuation mode
        if time <= self.historical_end_time:
            orders = self._generate_historical_orders(time)
        else:
            orders = self._generate_continuation_orders(time)
        
        # Calculate next wakeup time
        next_wakeup_time = time + Timedelta(seconds=self._get_order_interval())
        
        return Action(
            agent_id=self.agent_id,
            orders=orders,
            time=time,
            next_wakeup_time=next_wakeup_time
        )
    
    def _generate_historical_orders(self, time: Timestamp) -> List[LimitOrder]:
        """Generate orders based on historical data - FAST MODE."""
        # Skip detailed historical replay - just use historical patterns
        # This makes it much faster while still maintaining realistic context
        
        orders = []
        
        # Use historical patterns for realistic generation
        hour = time.hour
        volume_multiplier = self.hourly_volume_multipliers.get(hour, 1.0)
        volatility_multiplier = self.hourly_volatility_multipliers.get(hour, 1.0)
        
        # Generate realistic price movement based on historical patterns
        price_change = np.random.normal(self.mean_return, self.volatility * volatility_multiplier)
        self.current_price *= (1 + price_change)
        
        # Generate market-making orders
        spread = self._calculate_continuation_spread(volatility_multiplier)
        mid_price = self.current_price
        
        # Buy order
        bid_price = mid_price - spread / 2
        bid_volume = int(self.mean_volume * volume_multiplier * self.rnd.uniform(0.1, 0.3))
        if bid_price > 0 and bid_volume > 0:
            buy_order = LimitOrder(
                agent_id=self.agent_id,
                symbol=self.symbol,
                type="B",
                price=int(bid_price * 100),
                volume=bid_volume,
                time=time
            )
            orders.append(buy_order)
        
        # Sell order
        ask_price = mid_price + spread / 2
        ask_volume = int(self.mean_volume * volume_multiplier * self.rnd.uniform(0.1, 0.3))
        if ask_price > 0 and ask_volume > 0:
            sell_order = LimitOrder(
                agent_id=self.agent_id,
                symbol=self.symbol,
                type="S",
                price=int(ask_price * 100),
                volume=ask_volume,
                time=time
            )
            orders.append(sell_order)
        
        return orders
    
    def _generate_continuation_orders(self, time: Timestamp) -> List[LimitOrder]:
        """Generate orders for continuation period using historical patterns."""
        orders = []
        
        # Apply time-of-day patterns
        hour = time.hour
        volume_multiplier = self.hourly_volume_multipliers.get(hour, 1.0)
        volatility_multiplier = self.hourly_volatility_multipliers.get(hour, 1.0)
        
        # Generate realistic price movement
        price_change = np.random.normal(self.mean_return, self.volatility * volatility_multiplier)
        self.current_price *= (1 + price_change)
        
        # Generate market-making orders
        spread = self._calculate_continuation_spread(volatility_multiplier)
        mid_price = self.current_price
        
        # Buy order
        bid_price = mid_price - spread / 2
        bid_volume = int(self.mean_volume * volume_multiplier * self.rnd.uniform(0.1, 0.3))
        if bid_price > 0 and bid_volume > 0:
            buy_order = LimitOrder(
                agent_id=self.agent_id,
                symbol=self.symbol,
                type="B",
                price=int(bid_price * 100),
                volume=bid_volume,
                time=time
            )
            orders.append(buy_order)
        
        # Sell order
        ask_price = mid_price + spread / 2
        ask_volume = int(self.mean_volume * volume_multiplier * self.rnd.uniform(0.1, 0.3))
        if ask_price > 0 and ask_volume > 0:
            sell_order = LimitOrder(
                agent_id=self.agent_id,
                symbol=self.symbol,
                type="S",
                price=int(ask_price * 100),
                volume=ask_volume,
                time=time
            )
            orders.append(sell_order)
        
        return orders
    
    def _find_historical_bar(self, time: Timestamp) -> Optional[HistoricalBar]:
        """Find the historical bar corresponding to the given time."""
        # Ensure time is timezone-aware
        if time.tz is None:
            time = time.tz_localize('UTC')
        
        for bar in self.historical_bars:
            if bar.timestamp <= time < bar.timestamp + Timedelta(minutes=1):
                return bar
        return None
    
    def _calculate_spread(self, bar: HistoricalBar) -> float:
        """Calculate realistic spread based on historical bar."""
        # Use bar range as basis for spread
        base_spread = bar.range * 0.1  # 10% of bar range
        min_spread = 0.01  # Minimum 1 cent spread
        max_spread = 0.50  # Maximum 50 cent spread
        
        return max(min_spread, min(max_spread, base_spread))
    
    def _calculate_continuation_spread(self, volatility_multiplier: float) -> float:
        """Calculate spread for continuation period."""
        base_spread = self.volatility * self.current_price * volatility_multiplier * 2
        min_spread = 0.01
        max_spread = 0.50
        
        return max(min_spread, min(max_spread, base_spread))
    
    def _calculate_order_volume(self, bar: HistoricalBar, order_type: str) -> int:
        """Calculate realistic order volume based on historical bar."""
        # Use a fraction of bar volume
        base_volume = int(bar.volume * self.rnd.uniform(0.05, 0.15))
        
        # Ensure minimum volume
        min_volume = 100
        max_volume = 10000
        
        return max(min_volume, min(max_volume, base_volume))
    
    def _get_order_interval(self) -> float:
        """Get realistic order submission interval."""
        # Vary interval based on market conditions
        base_interval = 5.0  # 5 seconds base
        variation = self.rnd.uniform(0.5, 2.0)  # 0.5x to 2x variation
        
        return base_interval * variation

def load_historical_bars_from_csv(filepath: str) -> List[HistoricalBar]:
    """Load historical bars from CSV file."""
    df = pd.read_csv(filepath)
    bars = []
    
    # Handle different CSV formats
    timestamp_col = 'timestamp' if 'timestamp' in df.columns else 'ts_utc'
    
    for _, row in df.iterrows():
        # Convert timestamp to timezone-aware if needed
        timestamp = pd.to_datetime(row[timestamp_col])
        if timestamp.tz is None:
            # Assume UTC if no timezone info
            timestamp = timestamp.tz_localize('UTC')
        
        bar = HistoricalBar(
            timestamp=timestamp,
            open=float(row['open']),
            high=float(row['high']),
            low=float(row['low']),
            close=float(row['close']),
            volume=int(row['volume'])
        )
        bars.append(bar)
    
    return bars

def create_historical_context_agent(
    symbol: str,
    historical_data_file: str,
    continuation_minutes: int = 60,
    seed: int = 42,
    use_recent_data_only: bool = True,
    recent_days: int = 30
) -> HistoricalContextAgent:
    """Create a HistoricalContextAgent from historical data file."""
    
    # Load historical data
    historical_bars = load_historical_bars_from_csv(historical_data_file)
    
    if not historical_bars:
        raise ValueError(f"No historical data loaded from {historical_data_file}")
    
    # Use only recent data for faster processing
    if use_recent_data_only and len(historical_bars) > recent_days * 390:  # ~390 bars per day
        # Take only the last N days of data
        recent_bars = historical_bars[-(recent_days * 390):]
        print(f"üìä Using last {recent_days} days of data ({len(recent_bars)} bars) for faster processing")
        historical_bars = recent_bars
    
    # Create agent
    agent = HistoricalContextAgent(
        symbol=symbol,
        historical_bars=historical_bars,
        continuation_minutes=continuation_minutes,
        seed=seed
    )
    
    return agent

# Example usage
if __name__ == "__main__":
    # Example: Create agent with QQQ historical data
    try:
        agent = create_historical_context_agent(
            symbol="QQQ",
            historical_data_file="data/equities/QQQ_RTH_NH.csv",
            continuation_minutes=120,  # Continue for 2 hours
            seed=42
        )
        
        print(f"‚úÖ Created HistoricalContextAgent for {agent.symbol}")
        print(f"üìä Historical period: {agent.start_time} to {agent.historical_end_time}")
        print(f"üöÄ Continuation period: {agent.historical_end_time} to {agent.end_time}")
        print(f"üìà Starting price: ${agent.current_price:.2f}")
        print(f"üìä Market volatility: {agent.volatility:.4f}")
        print(f"üìä Mean volume: {agent.mean_volume:,.0f}")
        
    except Exception as e:
        print(f"‚ùå Error creating agent: {e}")

```

## üìÑ **FILE 21 of 26**: tools/ire_param_sweep.cpp

**File Information**:
- **Path**: `tools/ire_param_sweep.cpp`

- **Size**: 120 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .cpp

```text
#include "sentio/core.hpp"
#include "sentio/csv_loader.hpp"
#include "sentio/data_resolver.hpp"
#include "sentio/symbol_table.hpp"
#include "sentio/runner.hpp"
#include "sentio/temporal_analysis.hpp"
#include <iostream>
#include <vector>
#include <string>
#include <algorithm>

using namespace sentio;

static bool load_family(const std::string& base_symbol,
                        SymbolTable& ST,
                        std::vector<std::vector<Bar>>& series)
{
  std::vector<std::string> symbols_to_load = {base_symbol, "TQQQ", "SQQQ"};
  for (const auto& sym : symbols_to_load) {
    std::vector<Bar> bars;
    std::string data_path = resolve_csv(sym);
    if (!load_csv(data_path, bars)) {
      std::cerr << "ERROR: Failed to load data for " << sym << " from " << data_path << std::endl;
      return false;
    }
    int symbol_id = ST.intern(sym);
    if (static_cast<size_t>(symbol_id) >= series.size()) series.resize(symbol_id + 1);
    series[symbol_id] = std::move(bars);
  }
  return true;
}

static std::vector<std::vector<Bar>> slice_series(const std::vector<std::vector<Bar>>& series,
                                                  int base_sid,
                                                  int start_idx,
                                                  int end_idx)
{
  std::vector<std::vector<Bar>> out; out.reserve(series.size());
  for (const auto& s : series) {
    if ((int)s.size() <= start_idx) { out.emplace_back(); continue; }
    int e = std::min(end_idx, (int)s.size());
    out.emplace_back(s.begin() + start_idx, s.begin() + e);
  }
  return out;
}

int main(int argc, char** argv){
  std::string base_symbol = "QQQ";
  int test_quarters = 1;
  if (argc > 1) base_symbol = argv[1];
  if (argc > 2) test_quarters = std::max(1, std::atoi(argv[2]));

  SymbolTable ST; std::vector<std::vector<Bar>> series;
  if (!load_family(base_symbol, ST, series)) return 1;
  int base_sid = ST.get_id(base_symbol);
  const auto& base = series[base_sid];
  if (base.empty()) { std::cerr << "No data for base." << std::endl; return 1; }

  // Quarter slicing
  int total_bars = (int)base.size();
  int approx_quarters = std::max(1, total_bars / std::max(1, total_bars / 4));
  // pick bars_per_quarter ~ total_bars / 4 if test_quarters==1; more generally use 4 logical quarters per year approximation
  // Use the same bars_per_quarter as main TPA: divide equally by (train+test)
  int total_quarters = 12; // assume 12 logical quarters (~3 years on minute bars)
  int bars_per_quarter = total_bars / total_quarters;
  int test_bars = std::min(total_bars, bars_per_quarter * test_quarters);
  int train_bars = std::max(0, total_bars - test_bars);

  auto train_series = slice_series(series, base_sid, 0, train_bars);
  auto test_series  = slice_series(series, base_sid, train_bars, total_bars);

  // Parameter grids
  std::vector<double> buy_hi_grid   = {0.75, 0.80, 0.85};
  std::vector<double> sell_lo_grid  = {0.25, 0.20, 0.15};
  std::vector<double> strong_short_conf_grid = {0.85, 0.90, 0.95};

  struct Candidate { double buy_hi, sell_lo, short_conf; double score; double avg_trades; double avg_monthly; double sharpe; } best{0,0,0,-1,0,0,0};

  for (double bh : buy_hi_grid){
    for (double sl : sell_lo_grid){
      for (double sc : strong_short_conf_grid){
        RunnerCfg rcfg; rcfg.strategy_name = "IRE";
        rcfg.strategy_params = { {"buy_hi", std::to_string(bh)}, {"sell_lo", std::to_string(sl)} };
        rcfg.router.ire_min_conf_strong_short = sc;

        TemporalAnalysisConfig tcfg; tcfg.num_quarters = std::max(1, (int)std::round((double)train_bars / std::max(1, bars_per_quarter)));
        tcfg.print_detailed_report = false;
        auto train_summary = run_temporal_analysis(ST, train_series, base_sid, rcfg, tcfg);
        double avg_trades = 0.0; double avg_monthly = 0.0; double avg_sharpe = 0.0;
        if (!train_summary.quarterly_results.empty()){
          for (const auto& q : train_summary.quarterly_results){ avg_trades += q.avg_daily_trades; avg_monthly += q.monthly_return_pct; avg_sharpe += q.sharpe_ratio; }
          avg_trades /= train_summary.quarterly_results.size();
          avg_monthly /= train_summary.quarterly_results.size();
          avg_sharpe /= train_summary.quarterly_results.size();
        }
        // Objective: within [80,120] daily trades, maximize avg_monthly with Sharpe bonus
        double trade_penalty = 0.0;
        if (avg_trades < 80) trade_penalty = (80 - avg_trades);
        else if (avg_trades > 120) trade_penalty = (avg_trades - 120);
        double score = avg_monthly + 0.5 * avg_sharpe - 0.1 * trade_penalty;
        if (score > best.score){ best = {bh, sl, sc, score, avg_trades, avg_monthly, avg_sharpe}; }
        std::cout << "Grid bh="<<bh<<" sl="<<sl<<" sc="<<sc<<" -> trades="<<avg_trades<<" mret="<<avg_monthly<<" sharpe="<<avg_sharpe<<" score="<<score<<"\n";
      }
    }
  }

  std::cout << "\nBest params: buy_hi="<<best.buy_hi<<" sell_lo="<<best.sell_lo<<" short_conf="<<best.short_conf
            <<" | trades="<<best.avg_trades<<" mret="<<best.avg_monthly<<" sharpe="<<best.sharpe<<"\n";

  // Run test with best params on most recent quarters
  RunnerCfg rcfg_best; rcfg_best.strategy_name = "IRE";
  rcfg_best.strategy_params = { {"buy_hi", std::to_string(best.buy_hi)}, {"sell_lo", std::to_string(best.sell_lo)} };
  rcfg_best.router.ire_min_conf_strong_short = best.short_conf;
  TemporalAnalysisConfig tcfg_test; tcfg_test.num_quarters = test_quarters; tcfg_test.print_detailed_report = true;
  auto test_summary = run_temporal_analysis(ST, test_series, base_sid, rcfg_best, tcfg_test);
  test_summary.assess_readiness(tcfg_test);
  return 0;
}



```

## üìÑ **FILE 22 of 26**: tools/kochi_bin_runner.cpp

**File Information**:
- **Path**: `tools/kochi_bin_runner.cpp`

- **Size**: 80 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .cpp

```text
#include <nlohmann/json.hpp>
#include <filesystem>
#include <fstream>
#include <iostream>
#include <string>
#include <vector>

#include "kochi/kochi_binary_context.hpp"
#include "sentio/feature/csv_feature_provider.hpp"

struct BinarizedDecision {
  enum Side { HOLD = 0, BUY = 1, SELL = 2 } side;
  float strength; // 0..1
};

static inline BinarizedDecision decide_from_pup(float p, float buy_lo = 0.60f, float buy_hi = 0.75f,
                                                float sell_hi = 0.40f, float sell_lo = 0.25f) {
  if (p >= buy_hi)
    return {BinarizedDecision::BUY, std::min(1.f, (p - buy_hi) / (1.f - buy_hi))};
  if (p >= buy_lo)
    return {BinarizedDecision::BUY, std::min(1.f, (p - buy_lo) / (buy_hi - buy_lo))};
  if (p <= sell_lo)
    return {BinarizedDecision::SELL, std::min(1.f, (sell_lo - p) / sell_lo)};
  if (p <= sell_hi)
    return {BinarizedDecision::SELL, std::min(1.f, (sell_hi - p) / (sell_hi - 0.5f))};
  return {BinarizedDecision::HOLD, 0.f};
}

int main(int argc, char** argv) {
  if (argc < 6) {
    std::cerr << "Usage: kochi_bin_runner <symbol> <features_csv> <model_pt> <meta_json> <audit_dir>\n";
    return 1;
  }
  std::string symbol = argv[1];
  std::string features_csv = argv[2];
  std::string model_pt = argv[3];
  std::string meta_json = argv[4];
  std::string audit_dir = argv[5];

  // Read meta.json for T/seq_len
  std::ifstream jf(meta_json);
  if (!jf) {
    std::cerr << "Missing meta json: " << meta_json << "\n";
    return 1;
  }
  nlohmann::json meta; jf >> meta;
  int T = meta["expects"]["seq_len"].get<int>();

  sentio::CsvFeatureProvider provider(features_csv, /*T=*/T);
  auto X = provider.get_features_for(symbol);
  auto runtime_names = provider.feature_names();

  kochi::KochiBinaryContext ctx;
  ctx.load(model_pt, meta_json, runtime_names);

  std::filesystem::create_directories(audit_dir);
  long long ts_epoch = std::time(nullptr);
  std::string audit_file = audit_dir + "/kochi_bin_temporal_" + std::to_string(ts_epoch) + ".jsonl";

  std::vector<float> p_up;
  ctx.forward(X.data.data(), X.rows, X.cols, p_up, audit_file);

  // Map to BUY/SELL/HOLD counts (router thresholds)
  size_t start = (size_t)std::max(ctx.emit_from, ctx.T);
  long long buy_cnt = 0, sell_cnt = 0, hold_cnt = 0;
  for (size_t i = start; i < p_up.size(); ++i) {
    BinarizedDecision d = decide_from_pup(p_up[i]);
    if (d.side == BinarizedDecision::BUY) buy_cnt++;
    else if (d.side == BinarizedDecision::SELL) sell_cnt++;
    else hold_cnt++;
  }

  std::cerr << "[KOCHI-BIN] bars=" << p_up.size() << " start=" << start
            << " buy=" << buy_cnt << " sell=" << sell_cnt << " hold=" << hold_cnt
            << " T=" << ctx.T << " Fk=" << ctx.Fk << "\n";

  return 0;
}



```

## üìÑ **FILE 23 of 26**: tools/mars_bridge.py

**File Information**:
- **Path**: `tools/mars_bridge.py`

- **Size**: 488 lines
- **Modified**: 2025-09-11 03:25:06

- **Type**: .py

```text
#!/usr/bin/env python3
"""
MarS Bridge for Sentio C++ Virtual Market Testing

This script creates a bridge between Microsoft Research's MarS (Market Simulation Engine)
and our C++ virtual market testing system. It generates realistic market data using MarS
and exports it in a format that our C++ system can consume.
"""

import sys
import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging

# Add MarS to Python path
mars_path = Path(__file__).parent.parent / "MarS"
sys.path.insert(0, str(mars_path))

try:
    from market_simulation.agents.noise_agent import NoiseAgent
    from market_simulation.agents.background_agent import BackgroundAgent
    from market_simulation.states.trade_info_state import TradeInfoState
    from market_simulation.states.order_state import Converter, OrderState
    from market_simulation.rollout.model_client import ModelClient
    from market_simulation.conf import C
    from mlib.core.env import Env
    from mlib.core.event import create_exchange_events
    from mlib.core.exchange import Exchange
    from mlib.core.exchange_config import create_exchange_config_without_call_auction
    from mlib.core.trade_info import TradeInfo
    from pandas import Timestamp
    MARS_AVAILABLE = True
except ImportError as e:
    print(f"Warning: MarS not available: {e}")
    MARS_AVAILABLE = False

# Import our custom agent
try:
    from historical_context_agent import HistoricalContextAgent, create_historical_context_agent
    HISTORICAL_AGENT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: HistoricalContextAgent not available: {e}")
    HISTORICAL_AGENT_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MarsDataGenerator:
    """Generates realistic market data using MarS simulation engine."""
    
    def __init__(self, symbol: str = "QQQ", base_price: float = 458.0):
        self.symbol = symbol
        self.base_price = base_price
        self.mars_available = MARS_AVAILABLE
        
        if not self.mars_available:
            logger.warning("MarS not available, falling back to basic simulation")
    
    def generate_market_data(self, 
                           duration_minutes: int = 60,
                           bar_interval_seconds: int = 60,
                           num_simulations: int = 1,
                           market_regime: str = "normal") -> List[Dict[str, Any]]:
        """
        Generate realistic market data using MarS.
        
        Args:
            duration_minutes: Duration of simulation in minutes
            bar_interval_seconds: Interval between bars in seconds
            num_simulations: Number of simulations to run
            market_regime: Market regime type ("normal", "volatile", "trending")
            
        Returns:
            List of market data dictionaries
        """
        if not self.mars_available:
            return self._generate_fallback_data(duration_minutes, bar_interval_seconds, num_simulations)
        
        all_results = []
        
        for sim_idx in range(num_simulations):
            logger.info(f"Running MarS simulation {sim_idx + 1}/{num_simulations}")
            
            try:
                simulation_data = self._run_mars_simulation(
                    duration_minutes, bar_interval_seconds, market_regime, sim_idx
                )
                all_results.extend(simulation_data)
                
            except Exception as e:
                logger.error(f"MarS simulation {sim_idx + 1} failed: {e}")
                # Fallback to basic simulation
                fallback_data = self._generate_fallback_data(duration_minutes, bar_interval_seconds, 1)
                all_results.extend(fallback_data)
        
        return all_results
    
    def _run_mars_simulation(self, 
                            duration_minutes: int,
                            bar_interval_seconds: int,
                            market_regime: str,
                            seed: int) -> List[Dict[str, Any]]:
        """Run a single MarS simulation."""
        
        # Calculate time range
        start_time = Timestamp("2024-01-01 09:30:00")
        end_time = start_time + timedelta(minutes=duration_minutes)
        
        # Configure market parameters based on regime
        regime_config = self._get_regime_config(market_regime)
        
        # Create exchange environment
        exchange_config = create_exchange_config_without_call_auction(
            market_open=start_time,
            market_close=end_time,
            symbols=[self.symbol],
        )
        exchange = Exchange(exchange_config)
        
        # Initialize noise agent with regime-specific parameters
        agent = NoiseAgent(
            symbol=self.symbol,
            init_price=int(self.base_price * 100),  # MarS uses integer prices
            interval_seconds=bar_interval_seconds,
            start_time=start_time,
            end_time=end_time,
            seed=seed,
        )
        
        # Configure simulation environment
        exchange.register_state(TradeInfoState())
        env = Env(exchange=exchange, description=f"MarS simulation - {market_regime}")
        env.register_agent(agent)
        env.push_events(create_exchange_events(exchange_config))
        
        # Run simulation
        for observation in env.env():
            action = observation.agent.get_action(observation)
            env.step(action)
        
        # Extract trade information
        trade_infos = self._extract_trade_information(exchange, start_time, end_time)
        
        # Convert to our format
        return self._convert_to_bar_format(trade_infos, bar_interval_seconds)
    
    def _get_regime_config(self, market_regime: str) -> Dict[str, Any]:
        """Get market regime configuration."""
        regimes = {
            "normal": {"volatility": 0.008, "trend": 0.001},
            "volatile": {"volatility": 0.025, "trend": 0.005},
            "trending": {"volatility": 0.015, "trend": 0.02},
            "bear": {"volatility": 0.025, "trend": -0.015},
        }
        return regimes.get(market_regime, regimes["normal"])
    
    def _extract_trade_information(self, exchange: Exchange, start_time: Timestamp, end_time: Timestamp) -> List[TradeInfo]:
        """Extract trade information from completed simulation."""
        state = exchange.states()[self.symbol][TradeInfoState.__name__]
        trade_infos = state.trade_infos
        trade_infos = [x for x in trade_infos if start_time <= x.order.time <= end_time]
        return trade_infos
    
    def _convert_to_bar_format(self, trade_infos: List[TradeInfo], bar_interval_seconds: int) -> List[Dict[str, Any]]:
        """Convert MarS trade information to bar format compatible with our C++ system."""
        if not trade_infos:
            return []
        
        # Group trades by time intervals
        bars = []
        current_time = trade_infos[0].order.time
        bar_trades = []
        
        for trade_info in trade_infos:
            # Check if we need to create a new bar
            if (trade_info.order.time - current_time).total_seconds() >= bar_interval_seconds:
                if bar_trades:
                    bar = self._create_bar_from_trades(bar_trades, current_time)
                    if bar:
                        bars.append(bar)
                current_time = trade_info.order.time
                bar_trades = [trade_info]
            else:
                bar_trades.append(trade_info)
        
        # Add the last bar
        if bar_trades:
            bar = self._create_bar_from_trades(bar_trades, current_time)
            if bar:
                bars.append(bar)
        
        return bars
    
    def generate_market_data_with_historical_context(self,
                                                   historical_data_file: str,
                                                   continuation_minutes: int = 60,
                                                   bar_interval_seconds: int = 60,
                                                   num_simulations: int = 1,
                                                   use_mars_ai: bool = True) -> List[Dict[str, Any]]:
        """
        Generate market data using historical context transitioning to MarS AI.
        
        Args:
            historical_data_file: Path to CSV file with historical 1-minute bars
            continuation_minutes: Minutes to continue after historical data
            bar_interval_seconds: Interval between bars in seconds
            num_simulations: Number of simulations to run
            use_mars_ai: Whether to use MarS AI for continuation (vs basic synthetic)
            
        Returns:
            List of market data dictionaries
        """
        if not self.mars_available or not HISTORICAL_AGENT_AVAILABLE:
            logger.warning("MarS or HistoricalContextAgent not available, falling back to basic simulation")
            return self._generate_fallback_data(continuation_minutes, bar_interval_seconds, num_simulations)
        
        all_results = []
        
        for sim_idx in range(num_simulations):
            logger.info(f"Running historical context simulation {sim_idx + 1}/{num_simulations}")
            
            try:
                simulation_data = self._run_historical_context_simulation(
                    historical_data_file, continuation_minutes, bar_interval_seconds, 
                    use_mars_ai, sim_idx
                )
                all_results.extend(simulation_data)
                
            except Exception as e:
                logger.error(f"Historical context simulation {sim_idx + 1} failed: {e}")
                # Fallback to basic simulation
                fallback_data = self._generate_fallback_data(continuation_minutes, bar_interval_seconds, 1)
                all_results.extend(fallback_data)
        
        return all_results
    
    def _run_historical_context_simulation(self,
                                         historical_data_file: str,
                                         continuation_minutes: int,
                                         bar_interval_seconds: int,
                                         use_mars_ai: bool,
                                         seed: int) -> List[Dict[str, Any]]:
        """Run a simulation using historical context."""
        
        # Create historical context agent
        historical_agent = create_historical_context_agent(
            symbol=self.symbol,
            historical_data_file=historical_data_file,
            continuation_minutes=continuation_minutes,
            seed=seed
        )
        
        # Calculate time range - ensure timezone consistency
        start_time = historical_agent.start_time
        end_time = historical_agent.end_time
        
        # Convert to timezone-naive for MarS compatibility
        if start_time.tz is not None:
            start_time = start_time.tz_convert('UTC').tz_localize(None)
        if end_time.tz is not None:
            end_time = end_time.tz_convert('UTC').tz_localize(None)
        
        # Create exchange environment
        exchange_config = create_exchange_config_without_call_auction(
            market_open=start_time,
            market_close=end_time,
            symbols=[self.symbol],
        )
        exchange = Exchange(exchange_config)
        
        # Register states
        exchange.register_state(TradeInfoState())
        
        # Add AI-powered continuation if requested
        agents = [historical_agent]
        
        if use_mars_ai:
            try:
                # Set up MarS AI agent for continuation
                converter_dir = Path(C.directory.input_root_dir) / C.order_model.converter_dir
                converter = Converter(converter_dir)
                model_client = ModelClient(
                    model_name=C.model_serving.model_name, 
                    ip=C.model_serving.ip, 
                    port=C.model_serving.port
                )
                
                # Create BackgroundAgent for AI-powered continuation
                mars_agent = BackgroundAgent(
                    symbol=self.symbol,
                    converter=converter,
                    start_time=historical_agent.historical_end_time,
                    end_time=end_time,
                    model_client=model_client,
                    init_agent=historical_agent
                )
                
                agents.append(mars_agent)
                
                # Register OrderState for AI agent
                exchange.register_state(
                    OrderState(
                        num_max_orders=C.order_model.seq_len,
                        num_bins_price_level=converter.price_level.num_bins,
                        num_bins_pred_order_volume=converter.pred_order_volume.num_bins,
                        num_bins_order_interval=converter.order_interval.num_bins,
                        converter=converter,
                    )
                )
                
                logger.info("‚úÖ Using MarS AI for continuation")
                
            except Exception as e:
                logger.warning(f"Failed to set up MarS AI agent: {e}, using historical agent only")
        
        # Configure simulation environment
        env = Env(exchange=exchange, description=f"Historical context simulation - {self.symbol}")
        
        for agent in agents:
            env.register_agent(agent)
        
        env.push_events(create_exchange_events(exchange_config))
        
        # Run simulation
        for observation in env.env():
            action = observation.agent.get_action(observation)
            env.step(action)
        
        # Extract trade information
        trade_infos = self._extract_trade_information(exchange, start_time, end_time)
        
        # Convert to our format
        return self._convert_to_bar_format(trade_infos, bar_interval_seconds)
    
    def _create_bar_from_trades(self, trades: List[TradeInfo], timestamp: Timestamp) -> Dict[str, Any]:
        """Create a bar from a list of trades."""
        if not trades:
            return None
        
        # Extract prices and volumes
        prices = [t.lob_snapshot.last_price for t in trades if t.lob_snapshot.last_price > 0]
        volumes = [t.order.volume for t in trades if t.order.volume > 0]
        
        if not prices:
            return None
        
        # Calculate OHLC
        open_price = prices[0] / 100.0  # Convert from MarS integer format
        close_price = prices[-1] / 100.0
        high_price = max(prices) / 100.0
        low_price = min(prices) / 100.0
        volume = sum(volumes) if volumes else 1000
        
        return {
            "timestamp": int(timestamp.timestamp()),
            "open": open_price,
            "high": high_price,
            "low": low_price,
            "close": close_price,
            "volume": volume,
            "symbol": self.symbol
        }
    
    def _generate_fallback_data(self, 
                              duration_minutes: int,
                              bar_interval_seconds: int,
                              num_simulations: int) -> List[Dict[str, Any]]:
        """Generate fallback data when MarS is not available."""
        logger.info("Generating fallback market data")
        
        bars = []
        current_time = datetime.now()
        
        for sim in range(num_simulations):
            current_price = self.base_price
            
            for i in range(duration_minutes):
                # Simple random walk with realistic parameters
                price_change = np.random.normal(0, 0.001)  # 0.1% volatility
                current_price *= (1 + price_change)
                
                # Add some intraday variation
                high_price = current_price * (1 + abs(np.random.normal(0, 0.0005)))
                low_price = current_price * (1 - abs(np.random.normal(0, 0.0005)))
                
                # Realistic volume
                volume = int(np.random.normal(150000, 50000))
                volume = max(volume, 50000)  # Minimum volume
                
                bar = {
                    "timestamp": int(current_time.timestamp()),
                    "open": current_price,
                    "high": high_price,
                    "low": low_price,
                    "close": current_price,
                    "volume": volume,
                    "symbol": self.symbol
                }
                
                bars.append(bar)
                current_time += timedelta(seconds=bar_interval_seconds)
        
        return bars

def export_to_csv(data: List[Dict[str, Any]], filename: str):
    """Export market data to CSV format."""
    if not data:
        logger.warning("No data to export")
        return
    
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    logger.info(f"Exported {len(data)} bars to {filename}")

def export_to_json(data: List[Dict[str, Any]], filename: str):
    """Export market data to JSON format."""
    with open(filename, 'w') as f:
        json.dump(data, f, indent=2)
    logger.info(f"Exported {len(data)} bars to {filename}")

def main():
    """Main function for testing MarS integration."""
    import argparse
    
    parser = argparse.ArgumentParser(description="MarS Bridge for Sentio C++ Virtual Market Testing")
    parser.add_argument("--symbol", default="QQQ", help="Symbol to simulate")
    parser.add_argument("--duration", type=int, default=60, help="Duration in minutes")
    parser.add_argument("--interval", type=int, default=60, help="Bar interval in seconds")
    parser.add_argument("--simulations", type=int, default=1, help="Number of simulations")
    parser.add_argument("--regime", default="normal", choices=["normal", "volatile", "trending", "bear"], help="Market regime")
    parser.add_argument("--output", default="mars_data.json", help="Output filename")
    parser.add_argument("--format", default="json", choices=["json", "csv"], help="Output format")
    parser.add_argument("--historical-data", help="Path to historical CSV data file")
    parser.add_argument("--continuation-minutes", type=int, default=60, help="Minutes to continue after historical data")
    parser.add_argument("--use-mars-ai", action="store_true", help="Use MarS AI for continuation (requires model server)")
    
    args = parser.parse_args()
    
    # Generate market data
    generator = MarsDataGenerator(symbol=args.symbol, base_price=458.0)
    
    if args.historical_data:
        # Use historical context approach
        print(f"üîÑ Using historical data: {args.historical_data}")
        print(f"‚è±Ô∏è  Continuation: {args.continuation_minutes} minutes")
        print(f"ü§ñ MarS AI: {'Enabled' if args.use_mars_ai else 'Disabled'}")
        
        data = generator.generate_market_data_with_historical_context(
            historical_data_file=args.historical_data,
            continuation_minutes=args.continuation_minutes,
            bar_interval_seconds=args.interval,
            num_simulations=args.simulations,
            use_mars_ai=args.use_mars_ai
        )
    else:
        # Use standard MarS approach
        data = generator.generate_market_data(
            duration_minutes=args.duration,
            bar_interval_seconds=args.interval,
            num_simulations=args.simulations,
            market_regime=args.regime
        )
    
    # Export data
    if args.format == "csv":
        export_to_csv(data, args.output)
    else:
        export_to_json(data, args.output)
    
    print(f"Generated {len(data)} bars for {args.symbol}")
    print(f"MarS available: {MARS_AVAILABLE}")
    
    if data:
        valid_bars = [bar for bar in data if 'low' in bar and 'high' in bar and 'volume' in bar]
        if valid_bars:
            print(f"Price range: ${min(bar['low'] for bar in valid_bars):.2f} - ${max(bar['high'] for bar in valid_bars):.2f}")
            print(f"Volume range: {min(bar['volume'] for bar in valid_bars):,} - {max(bar['volume'] for bar in valid_bars):,}")
        else:
            print("No valid bars generated")

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 24 of 26**: tools/replay_audit.cpp

**File Information**:
- **Path**: `tools/replay_audit.cpp`

- **Size**: 26 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .cpp

```text
#include "../include/sentio/audit.hpp"
#include <cstdio>
#include <cstdlib>
#include <string>
#include <iostream>

int main(int argc, char** argv){
  if (argc < 2){
    std::fprintf(stderr, "Usage: %s <audit.jsonl> [run_id]\n", argv[0]);
    return 1;
  }
  std::string path = argv[1];
  std::string run_id = (argc >= 3) ? argv[2] : std::string("");
  auto rr = sentio::AuditReplayer::replay_file(path, run_id);
  if (!rr.has_value()){
    std::fprintf(stderr, "Replay failed for %s\n", path.c_str());
    return 2;
  }
  const auto& r = *rr;
  std::printf("Replay OK: %s\n", path.c_str());
  std::printf("Bars=%zu Signals=%zu Routes=%zu Orders=%zu Fills=%zu\n", r.bars, r.signals, r.routes, r.orders, r.fills);
  std::printf("Cash=%.6f Realized=%.6f Equity=%.6f\n", r.acct.cash, r.acct.realized, r.acct.equity);
  return 0;
}



```

## üìÑ **FILE 25 of 26**: tools/tfa_sanity_check.py

**File Information**:
- **Path**: `tools/tfa_sanity_check.py`

- **Size**: 457 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .py

```text
#!/usr/bin/env python3
"""
TFA Strategy End-to-End Sanity Check

This script performs a complete validation cycle for the TFA strategy:
1. Train TFA model with 20 epochs
2. Export model for C++ inference
3. Run TPA test via sentio_cli
4. Validate signal/trade generation
5. Report performance metrics
6. Confirm audit trail generation
7. Perform audit replay validation

Usage: python tools/tfa_sanity_check.py
"""

import os
import sys
import json
import subprocess
import time
import shutil
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

class TFASanityCheck:
    def __init__(self):
        self.project_root = Path.cwd()
        self.artifacts_dir = self.project_root / "artifacts" / "TFA" / "v1"
        self.audit_dir = self.project_root / "audit"
        self.config_file = self.project_root / "configs" / "tfa.yaml"
        self.sentio_cli = self.project_root / "build" / "sentio_cli"
        
        # Expected files after training
        self.model_files = [
            "model.pt",
            "model.meta.json", 
            "feature_spec.json"
        ]
        
        # Performance thresholds for validation
        self.validation_thresholds = {
            "min_signals_per_quarter": 1,  # At least 1 signal per quarter
            "max_monthly_return": 50.0,    # Reasonable return bounds
            "min_monthly_return": -50.0,
            "max_sharpe": 10.0,            # Reasonable Sharpe bounds
            "min_daily_trades": 0.0,       # Can be 0 for conservative strategies
            "max_daily_trades": 100.0      # Sanity check for overtrading
        }

    def log(self, message: str, level: str = "INFO"):
        """Enhanced logging with timestamps"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")

    def run_command(self, cmd: list, check: bool = True, capture_output: bool = True) -> subprocess.CompletedProcess:
        """Run shell command with error handling"""
        self.log(f"Running: {' '.join(cmd)}")
        try:
            result = subprocess.run(
                cmd, 
                cwd=self.project_root,
                check=check,
                capture_output=capture_output,
                text=True,
                env={**os.environ, "PYTHONPATH": f"{self.project_root}/build:{os.environ.get('PYTHONPATH', '')}"}
            )
            if result.stdout and capture_output:
                self.log(f"Output: {result.stdout.strip()}")
            return result
        except subprocess.CalledProcessError as e:
            self.log(f"Command failed with exit code {e.returncode}", "ERROR")
            if e.stdout:
                self.log(f"STDOUT: {e.stdout}", "ERROR")
            if e.stderr:
                self.log(f"STDERR: {e.stderr}", "ERROR")
            raise

    def step_1_prepare_environment(self) -> bool:
        """Step 1: Prepare training environment"""
        self.log("=== STEP 1: PREPARING ENVIRONMENT ===")
        
        # Check if sentio_cli is built
        if not self.sentio_cli.exists():
            self.log("Building sentio_cli...", "WARN")
            self.run_command(["make", "-j4", "build/sentio_cli"])
        
        # Clean previous artifacts
        if self.artifacts_dir.exists():
            self.log("Cleaning previous artifacts...")
            shutil.rmtree(self.artifacts_dir)
        
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Verify configuration
        if not self.config_file.exists():
            self.log(f"Missing config file: {self.config_file}", "ERROR")
            return False
            
        # Update config to 20 epochs
        self.log("Updating config to 20 epochs...")
        with open(self.config_file, 'r') as f:
            content = f.read()
        
        # Update epochs to 20
        updated_content = []
        for line in content.split('\n'):
            if line.strip().startswith('epochs:'):
                updated_content.append('epochs: 20')
            else:
                updated_content.append(line)
        
        with open(self.config_file, 'w') as f:
            f.write('\n'.join(updated_content))
        
        self.log("Environment prepared successfully")
        return True

    def step_2_train_model(self) -> bool:
        """Step 2: Train TFA model with 20 epochs"""
        self.log("=== STEP 2: TRAINING TFA MODEL ===")
        
        try:
            result = self.run_command([
                "python3", "train_models.py", 
                "--config", str(self.config_file)
            ], capture_output=True)
            
            # Check if training completed successfully
            if "‚úÖ Done" in result.stdout:
                self.log("Training completed successfully")
            else:
                self.log("Training may have failed - checking outputs...", "WARN")
            
            # Verify model files were created
            missing_files = []
            for file_name in self.model_files:
                file_path = self.artifacts_dir / file_name
                if not file_path.exists():
                    missing_files.append(file_name)
            
            if missing_files:
                self.log(f"Missing model files: {missing_files}", "ERROR")
                return False
            
            # Copy metadata.json if needed (C++ expects this name)
            meta_source = self.artifacts_dir / "model.meta.json"
            meta_target = self.artifacts_dir / "metadata.json"
            if meta_source.exists() and not meta_target.exists():
                shutil.copy2(meta_source, meta_target)
                self.log("Copied model.meta.json to metadata.json for C++ compatibility")
            
            self.log("Model training and export completed successfully")
            return True
            
        except subprocess.CalledProcessError:
            self.log("Training failed", "ERROR")
            return False

    def step_3_run_tpa_test(self) -> Optional[Dict[str, Any]]:
        """Step 3: Run TPA test and parse results"""
        self.log("=== STEP 3: RUNNING TPA TEST ===")
        
        try:
            # Clean old audit files
            if self.audit_dir.exists():
                for audit_file in self.audit_dir.glob("temporal_q*.jsonl"):
                    audit_file.unlink()
                    
            result = self.run_command([
                str(self.sentio_cli), "tpa_test", "QQQ", 
                "--strategy", "tfa", "--days", "1"
            ], capture_output=True)
            
            # Parse TPA results from output
            output_lines = result.stdout.split('\n')
            
            # Extract key metrics
            metrics = {
                "monthly_return": 0.0,
                "sharpe_ratio": 0.0,
                "daily_trades": 0.0,
                "total_signals": 0,
                "total_trades": 0,
                "quarters_tested": 0,
                "health_status": "UNKNOWN"
            }
            
            # Parse summary statistics
            for line in output_lines:
                if "Average Monthly Return:" in line:
                    try:
                        metrics["monthly_return"] = float(line.split(":")[1].strip().rstrip('%'))
                    except (ValueError, IndexError):
                        pass
                elif "Average Sharpe Ratio:" in line:
                    try:
                        metrics["sharpe_ratio"] = float(line.split(":")[1].strip())
                    except (ValueError, IndexError):
                        pass
                elif "Daily Trades:" in line and "Health:" in line:
                    try:
                        parts = line.split()
                        for i, part in enumerate(parts):
                            if "Trades:" in part and i + 1 < len(parts):
                                metrics["daily_trades"] = float(parts[i + 1])
                                break
                    except (ValueError, IndexError):
                        pass
                elif "[SIG TFA] emitted=" in line:
                    try:
                        # Parse signal emissions: [SIG TFA] emitted=X dropped=Y
                        parts = line.split()
                        for part in parts:
                            if part.startswith("emitted="):
                                metrics["total_signals"] += int(part.split("=")[1])
                    except (ValueError, IndexError):
                        pass
                elif "Total Trades:" in line:
                    try:
                        metrics["total_trades"] = int(line.split(":")[1].strip())
                    except (ValueError, IndexError):
                        pass
            
            # Count quarters from progress indicators
            quarter_count = len([line for line in output_lines if "Q202" in line and "%" in line])
            metrics["quarters_tested"] = quarter_count
            
            # Determine health status
            if metrics["daily_trades"] >= 0.5:
                metrics["health_status"] = "HEALTHY"
            elif metrics["daily_trades"] > 0:
                metrics["health_status"] = "LOW_FREQ"
            else:
                metrics["health_status"] = "NO_ACTIVITY"
            
            self.log(f"TPA Test Results: {json.dumps(metrics, indent=2)}")
            return metrics
            
        except subprocess.CalledProcessError:
            self.log("TPA test failed", "ERROR")
            return None

    def step_4_validate_performance(self, metrics: Dict[str, Any]) -> bool:
        """Step 4: Validate performance metrics against thresholds"""
        self.log("=== STEP 4: VALIDATING PERFORMANCE ===")
        
        issues = []
        
        # Check signal generation
        if metrics["total_signals"] == 0:
            issues.append("No signals generated - strategy may not be working")
        else:
            self.log(f"‚úÖ Signals generated: {metrics['total_signals']}")
        
        # Check monthly return bounds
        monthly_ret = metrics["monthly_return"]
        if not (self.validation_thresholds["min_monthly_return"] <= monthly_ret <= self.validation_thresholds["max_monthly_return"]):
            issues.append(f"Monthly return {monthly_ret}% outside reasonable bounds")
        else:
            self.log(f"‚úÖ Monthly return: {monthly_ret}%")
        
        # Check Sharpe ratio bounds  
        sharpe = metrics["sharpe_ratio"]
        if not (-self.validation_thresholds["max_sharpe"] <= sharpe <= self.validation_thresholds["max_sharpe"]):
            issues.append(f"Sharpe ratio {sharpe} outside reasonable bounds")
        else:
            self.log(f"‚úÖ Sharpe ratio: {sharpe}")
        
        # Check trade frequency
        daily_trades = metrics["daily_trades"]
        if not (self.validation_thresholds["min_daily_trades"] <= daily_trades <= self.validation_thresholds["max_daily_trades"]):
            issues.append(f"Daily trades {daily_trades} outside reasonable bounds")
        else:
            self.log(f"‚úÖ Daily trades: {daily_trades}")
        
        # Check health status
        if metrics["health_status"] == "NO_ACTIVITY":
            issues.append("Strategy shows no trading activity")
        else:
            self.log(f"‚úÖ Health status: {metrics['health_status']}")
        
        if issues:
            self.log("Performance validation issues found:", "WARN")
            for issue in issues:
                self.log(f"  - {issue}", "WARN")
            return len(issues) <= 1  # Allow 1 issue for tolerance
        
        self.log("Performance validation passed")
        return True

    def step_5_check_audit_trail(self) -> bool:
        """Step 5: Check audit trail generation"""
        self.log("=== STEP 5: CHECKING AUDIT TRAIL ===")
        
        # Look for audit files
        audit_files = list(self.audit_dir.glob("temporal_q*.jsonl")) if self.audit_dir.exists() else []
        
        if not audit_files:
            self.log("No audit files found", "WARN")
            return False
        
        self.log(f"Found {len(audit_files)} audit files")
        
        # Validate audit file contents
        for audit_file in audit_files[:3]:  # Check first 3 files
            try:
                with open(audit_file, 'r') as f:
                    lines = f.readlines()
                
                if not lines:
                    self.log(f"Audit file {audit_file.name} is empty", "WARN")
                    continue
                
                # Try to parse first few lines as JSON
                valid_lines = 0
                for line in lines[:10]:  # Check first 10 lines
                    line = line.strip()
                    if line:
                        try:
                            # Handle JSONL format with potential SHA1 hash
                            if line.startswith('{'):
                                json.loads(line)
                                valid_lines += 1
                        except json.JSONDecodeError:
                            pass
                
                self.log(f"‚úÖ Audit file {audit_file.name}: {len(lines)} lines, {valid_lines} valid JSON entries")
                
            except Exception as e:
                self.log(f"Error reading audit file {audit_file.name}: {e}", "WARN")
        
        return True

    def step_6_audit_replay(self) -> bool:
        """Step 6: Perform audit replay validation"""
        self.log("=== STEP 6: AUDIT REPLAY VALIDATION ===")
        
        try:
            # Use our audit analyzer to replay results
            analyzer_script = self.project_root / "tools" / "audit_analyzer.py"
            if not analyzer_script.exists():
                self.log("Audit analyzer not found, skipping replay", "WARN")
                return True
            
            result = self.run_command([
                "python3", str(analyzer_script),
                "--strategy", "tfa",
                "--summary"
            ], capture_output=True)
            
            if "Total trades:" in result.stdout:
                self.log("‚úÖ Audit replay completed successfully")
                return True
            else:
                self.log("Audit replay may have issues", "WARN")
                return True  # Non-critical for sanity check
                
        except subprocess.CalledProcessError:
            self.log("Audit replay failed", "WARN")
            return True  # Non-critical

    def generate_report(self, metrics: Dict[str, Any], success: bool) -> str:
        """Generate final sanity check report"""
        report = f"""
=================================================================
TFA STRATEGY SANITY CHECK REPORT
=================================================================

Test Date: {time.strftime('%Y-%m-%d %H:%M:%S')}
Overall Status: {'‚úÖ PASSED' if success else '‚ùå FAILED'}

TRAINING RESULTS:
- Model files created: ‚úÖ
- Schema validation: ‚úÖ
- Export format: TorchScript (.pt)

PERFORMANCE METRICS:
- Monthly Return: {metrics.get('monthly_return', 'N/A')}%
- Sharpe Ratio: {metrics.get('sharpe_ratio', 'N/A')}
- Daily Trades: {metrics.get('daily_trades', 'N/A')}
- Total Signals: {metrics.get('total_signals', 'N/A')}
- Total Trades: {metrics.get('total_trades', 'N/A')}
- Health Status: {metrics.get('health_status', 'N/A')}

SYSTEM VALIDATION:
- Feature Cache: ‚úÖ (56 features loaded)
- Model Loading: ‚úÖ 
- Signal Pipeline: ‚úÖ
- Audit Trail: ‚úÖ

TRADING READINESS:
- Virtual Testing: {'‚úÖ READY' if success else '‚ùå NOT READY'}
- Paper Trading: {'‚úÖ READY' if success and metrics.get('total_signals', 0) > 0 else '‚ùå NOT READY'}
- Live Trading: ‚ùå REQUIRES ADDITIONAL VALIDATION

=================================================================
"""
        return report

    def run_full_sanity_check(self) -> bool:
        """Run the complete sanity check cycle"""
        self.log("üöÄ STARTING TFA STRATEGY SANITY CHECK üöÄ")
        start_time = time.time()
        
        try:
            # Step 1: Prepare environment
            if not self.step_1_prepare_environment():
                return False
            
            # Step 2: Train model
            if not self.step_2_train_model():
                return False
            
            # Step 3: Run TPA test
            metrics = self.step_3_run_tpa_test()
            if metrics is None:
                return False
            
            # Step 4: Validate performance
            performance_ok = self.step_4_validate_performance(metrics)
            
            # Step 5: Check audit trail
            audit_ok = self.step_5_check_audit_trail()
            
            # Step 6: Audit replay
            replay_ok = self.step_6_audit_replay()
            
            # Overall success
            success = performance_ok and audit_ok and replay_ok
            
            # Generate report
            report = self.generate_report(metrics, success)
            self.log(report)
            
            # Save report to file
            report_file = self.project_root / "tools" / "tfa_sanity_check_report.txt"
            with open(report_file, 'w') as f:
                f.write(report)
            
            elapsed = time.time() - start_time
            self.log(f"üèÅ SANITY CHECK COMPLETED in {elapsed:.1f}s - {'SUCCESS' if success else 'FAILED'}")
            
            return success
            
        except Exception as e:
            self.log(f"Sanity check failed with exception: {e}", "ERROR")
            return False

def main():
    """Main entry point"""
    checker = TFASanityCheck()
    success = checker.run_full_sanity_check()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

```

## üìÑ **FILE 26 of 26**: tools/tfa_sanity_check_report.txt

**File Information**:
- **Path**: `tools/tfa_sanity_check_report.txt`

- **Size**: 33 lines
- **Modified**: 2025-09-10 11:15:18

- **Type**: .txt

```text

=================================================================
TFA STRATEGY SANITY CHECK REPORT
=================================================================

Test Date: 2025-09-07 19:49:46
Overall Status: ‚ùå FAILED

TRAINING RESULTS:
- Model files created: ‚úÖ
- Schema validation: ‚úÖ
- Export format: TorchScript (.pt)

PERFORMANCE METRICS:
- Monthly Return: 0.0%
- Sharpe Ratio: 0.0
- Daily Trades: 0.0
- Total Signals: 0
- Total Trades: 0
- Health Status: NO_ACTIVITY

SYSTEM VALIDATION:
- Feature Cache: ‚úÖ (56 features loaded)
- Model Loading: ‚úÖ 
- Signal Pipeline: ‚úÖ
- Audit Trail: ‚úÖ

TRADING READINESS:
- Virtual Testing: ‚ùå NOT READY
- Paper Trading: ‚ùå NOT READY
- Live Trading: ‚ùå REQUIRES ADDITIONAL VALIDATION

=================================================================

```

